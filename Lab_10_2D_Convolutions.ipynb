{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab 10 - 2D Convolutions.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CBravoR/AdvancedAnalyticsLabs/blob/master/Lab_10_2D_Convolutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "4KefNczLWrL8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2D Convolutional Neural Networks"
      ]
    },
    {
      "metadata": {
        "id": "neKsl_mxWrL9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this lab, we will train two models:\n",
        "\n",
        "1. A traditional VGG16 model, where we will train the last couple of layers to adapt it to our problem.\n",
        "\n",
        "2. A more humble model that we design here.\n",
        "\n",
        "The problem is to predict one of 6 categories of pictures, whether they are. 'buildings', 'forest', 'glacier', 'mountain', 'sea' or 'street'. This problem comes from an Intel Image Classification challenge and is available [here](https://www.kaggle.com/puneet6060/intel-image-classification/version/2).\n",
        "\n",
        "First, we load the data."
      ]
    },
    {
      "metadata": {
        "id": "kLUGY4T_W4Z1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!gdown 'https://drive.google.com/uc?export=download&id=1HEB7JHl6uSANiENvaHBxhzmkkZ9YlhEC'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ha2b0IUlXibC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!unzip IntelClassification.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oYYauoNEWrL-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## VGG16\n",
        "\n",
        "The VGG 16 model is a classic model in Deep Learning. It is a 16 layer model, following the structure that we discussed in the lectures.\n",
        "\n",
        "This model was trained over the ImageNet data, thus looking to classify among 1000 different types of objects, over a very large database of images. We can leverage these already-trained weights, and  adapt just the last few layers for our purposes.\n",
        "\n",
        "We start by loading the VGG16 model. Keras comes pre-packaged with a series of models, loaded into the [Applications library](https://keras.io/applications/). We start by first loading the model on-the-fly using the library. We can check the options of the model in the options of the fuction [VGG16](https://keras.io/applications/#vgg16).\n",
        "\n",
        "We also need a package that allows for an efficient storage of the model using a binary format. The package is called [h5py](https://www.h5py.org/) and also allows for storing your pre-trained models. You can read a tutorial for this [here](https://machinelearningmastery.com/save-load-keras-deep-learning-models/)."
      ]
    },
    {
      "metadata": {
        "id": "vD3pJQRDWrMB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import h5py as h5py\n",
        "import PIL\n",
        "# Others\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# For AUC estimation and ROC plots\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "\n",
        "# Plots\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "el4B7dJ_WrMD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "model = VGG16(weights = 'imagenet',    # The weights from the ImageNet competition\n",
        "              include_top = False,     # Do not include the top layer, which classifies.\n",
        "              input_shape= (224, 224, 3) # Input shape. Three channels, and BGR (NOT RGB!!!)\n",
        "             )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0iaFjronWrMH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This will download the model and save it to our unoriginally named variable model. It is a pretty beefy download, at around 500MB. Luckly we have fast internet in the lab :)\n",
        "\n",
        "We can now check the details of the model as usual."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "dzEtNZi0WrMH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import plot_model\n",
        "from IPython.display import Image\n",
        "\n",
        "plot_model(model, show_shapes=True, show_layer_names=True, to_file='GraphModel.png')\n",
        "Image(retina=True, filename='GraphModel.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xDVzqhVBWrMJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "At this point, every single parameter is trainable. We don't need this, as we want to use the parameters that come with the model. We will create a parallel model to store the new trainable layers, and then set all of these layers as untrainable. We will finally add a Dense layer with 128 neurons, plus a Dense layer with two classes."
      ]
    },
    {
      "metadata": {
        "id": "Wn6IyJsGWrMK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout, Flatten, Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QJLpTDWzWrMM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create new model\n",
        "CBModel = Sequential()\n",
        "\n",
        "# Copy the layers to our new model. This needs to be done as there is a bug in Keras.\n",
        "for layer in model.layers:\n",
        "    CBModel.add(layer)\n",
        "\n",
        "# Set the layers as untrainable\n",
        "for layer in CBModel.layers:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aVqY193_WrMP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "CBModel.add(Flatten(input_shape=model.output_shape[1:]))\n",
        "CBModel.add(Dense(64, activation = 'relu'))\n",
        "CBModel.add(Dropout(0.5))\n",
        "CBModel.add(Dense(6, activation = 'softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q3jEAWLcWrMR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "CBModel.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9I2NBj-WWrMT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "CBModel.compile(loss='categorical_crossentropy', # This is NOT a binary problem!\n",
        "              optimizer=optimizers.adam(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "94fkn6CvWrMV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ready! We can now load the data. We have our set of pictures ready for this example.\n",
        "\n",
        "We will first build two image generators (one for testing and one for training), which will generate new samples on the fly using our pictures as input. This is the same that we say in the lectures."
      ]
    },
    {
      "metadata": {
        "id": "8c4AnfLGWrMW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# prepare data augmentation configuration\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1. / 255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True)\n",
        "\n",
        "# We will use a batch size of 32. Depends on RAM of GPU.\n",
        "batch_size = 32\n",
        "\n",
        "# Train data generator. We point to the training directory!\n",
        "train_data_dir = 'IntelClassification/seg_train'\n",
        "\n",
        "# VGG requires 224 x 224 images.\n",
        "(img_height, img_width) = (224, 224)\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    train_data_dir, # Where are the pics\n",
        "    target_size=(img_height, img_width), # What size should they be\n",
        "    batch_size=batch_size, # Size of batch\n",
        "    class_mode='categorical' # Class mode, whether 'binary' or 'categorical'\n",
        ")\n",
        "\n",
        "# Validation data generator.\n",
        "validation_data_dir = 'IntelClassification/seg_test'\n",
        "validation_generator = datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ELxVqH7IWrMY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now the system is ready to train from the images that we have loaded. We now feed the generators to the model, and ask to train for a certain number of epochs."
      ]
    },
    {
      "metadata": {
        "id": "Pk9tyeE1WrMY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Number of epochs\n",
        "epochs = 20\n",
        "\n",
        "# Train!\n",
        "CBModel.fit_generator(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    steps_per_epoch = 32, # Usually cases / batch_size. Reduced to 32 so it runs faster.\n",
        "    validation_steps = 32 # Number of validation steps.\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iFOBLbK9WrMa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The model is able to learn quite well! Let's check the convergence plot."
      ]
    },
    {
      "metadata": {
        "id": "kD5qzVeBvEsK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss = CBModel.history.history['loss']\n",
        "val_loss = CBModel.history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uarSijL0WrMb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training of simple(r) network.\n",
        "\n",
        "Let's try a simpler architecture. A sequence of three Separable Convlutions, of 64 filters, followed  by a batch normalization layer. We will then add a max_pooling of 3x3 and our common dense layer with a softmax output.\n",
        "\n",
        "Note: Keras does not yet have BatchRenormalization implemented. It has been announced that it will be included later. For now, you can use the proposed implementation available [here](https://github.com/titu1994/BatchRenormalization)."
      ]
    },
    {
      "metadata": {
        "id": "YyPV3iQlWrMb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import SeparableConv2D, BatchNormalization, Input, MaxPooling2D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g2gazvo6WrMd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SimpleModel = Sequential()\n",
        "\n",
        "# Our blocks\n",
        "SimpleModel.add(SeparableConv2D(filters=64, kernel_size=(2, 2), input_shape = (224, 224, 3),\n",
        "                               activation = 'relu'))\n",
        "SimpleModel.add(BatchNormalization())\n",
        "\n",
        "SimpleModel.add(SeparableConv2D(filters=64, kernel_size=(2, 2), activation = 'relu'))\n",
        "SimpleModel.add(BatchNormalization())\n",
        "\n",
        "SimpleModel.add(SeparableConv2D(filters=64, kernel_size=(2, 2), activation = 'relu'))\n",
        "SimpleModel.add(BatchNormalization())\n",
        "\n",
        "SimpleModel.add(MaxPooling2D(pool_size = (3, 3)))\n",
        "\n",
        "# Second group. More filters per layer.\n",
        "SimpleModel.add(SeparableConv2D(filters=128, kernel_size=(2, 2), activation = 'relu'))\n",
        "SimpleModel.add(BatchNormalization())\n",
        "\n",
        "SimpleModel.add(SeparableConv2D(filters=128, kernel_size=(2, 2), activation = 'relu'))\n",
        "SimpleModel.add(BatchNormalization())\n",
        "\n",
        "SimpleModel.add(SeparableConv2D(filters=128, kernel_size=(2, 2), activation = 'relu'))\n",
        "SimpleModel.add(BatchNormalization())\n",
        "\n",
        "SimpleModel.add(MaxPooling2D(pool_size = (3, 3)))\n",
        "\n",
        "# Third group. More filters per layer.\n",
        "SimpleModel.add(SeparableConv2D(filters=256, kernel_size=(2, 2), activation = 'relu'))\n",
        "SimpleModel.add(BatchNormalization())\n",
        "\n",
        "SimpleModel.add(SeparableConv2D(filters=256, kernel_size=(2, 2), activation = 'relu'))\n",
        "SimpleModel.add(BatchNormalization())\n",
        "\n",
        "SimpleModel.add(SeparableConv2D(filters=256, kernel_size=(2, 2), activation = 'relu'))\n",
        "SimpleModel.add(BatchNormalization())\n",
        "\n",
        "SimpleModel.add(MaxPooling2D(pool_size = (3, 3)))\n",
        "\n",
        "# The prediction layers\n",
        "SimpleModel.add(Flatten())\n",
        "SimpleModel.add(Dense(128, activation = 'relu'))\n",
        "SimpleModel.add(Dropout(0.5))\n",
        "SimpleModel.add(Dense(128, activation = 'relu'))\n",
        "SimpleModel.add(Dropout(0.5))\n",
        "SimpleModel.add(Dense(6, activation = 'softmax')) # Softmax, not sigmoid!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GuY5kj_6WrMf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SimpleModel.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FK3pIX8uWrMi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now train using the same generators as before."
      ]
    },
    {
      "metadata": {
        "id": "sGnbr7MpWrMj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SimpleModel.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.adam(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Stx4x5J7WrMn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SimpleModel.fit_generator(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=validation_generator,\n",
        "    steps_per_epoch = 32, # Generate 3 batches\n",
        "    validation_steps = 32\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5pszZE38Cw0q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss = SimpleModel.history.history['loss']\n",
        "val_loss = SimpleModel.history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q2RFgJfDWrMq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This model is doing something, but it probably needs more complexity for it to be able to learn efficiently the weights. Or maybe we need to give it more epochs for it to converge correctly!\n",
        "\n",
        "As self-study, experiment with these configurations, train for longer, and try to get a model that can learn correctly."
      ]
    },
    {
      "metadata": {
        "id": "BBQ9y9kUWrMr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualizing learning\n",
        "\n",
        "As a final example. We will visualize the learning, to detect exactly what is happening.\n",
        "\n",
        "We start by importing a single image from the pos folder."
      ]
    },
    {
      "metadata": {
        "id": "vdG3olIZWrMr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "img_path = 'IntelClassification/seg_test/buildings/24120.jpg'\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "img_tensor = image.img_to_array(img)\n",
        "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
        "img_tensor /= 255.\n",
        "\n",
        "\n",
        "plt.imshow(img_tensor[0])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xbba19xgWrMt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will now create a model that will have only the activations from the first layer."
      ]
    },
    {
      "metadata": {
        "id": "nm6u7BnfWrMu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import models\n",
        "\n",
        "# Get output from layer 1.\n",
        "layer_outputs = CBModel.layers[1].get_output_at(1)\n",
        "\n",
        "# Create a very simple model that only \n",
        "activation_model = models.Model(inputs=CBModel.input, outputs=layer_outputs)\n",
        "activations = activation_model.predict(img_tensor)\n",
        "\n",
        "# get the activations\n",
        "first_layer_activation = activations[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JzV_edecWrMx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "first_layer_activation.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tRBYta6pWrMz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The shape is a picture, of size 224 x 224, with 64 filters. We can now visualize any of these, for example, number 4.\n"
      ]
    },
    {
      "metadata": {
        "id": "ECBnPhjbWrM0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.matshow(first_layer_activation[:, :, 5], cmap='viridis')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Za6Fu41GWrM1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now try to overlay exactly what is being used to detect people. I will first calculate the probability that is being used."
      ]
    },
    {
      "metadata": {
        "id": "414R6xsTWrM2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "preds = [1, 0, 0, 0, 0, 0] - CBModel.predict(img_tensor)\n",
        "CB_output = CBModel.output\n",
        "preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xz86S1QHWrM3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we will extract the final layer, and run a gradient ascent algorithm: Basically we will look for the direction that the model used to get to its decisions. This requires to do some iterations that we won't explain in detail. Please refer to chapter 5.4 of Chollet's book to check the details."
      ]
    },
    {
      "metadata": {
        "id": "zqpZ5XU8WrM4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras.backend as K\n",
        "\n",
        "# Import the last layer. We use the name from the model summary.\n",
        "last_conv_layer = CBModel.get_layer('block5_conv3')\n",
        "\n",
        "# Get the gradients\n",
        "grads = K.gradients(CB_output, last_conv_layer.get_output_at(1))[0]\n",
        "\n",
        "# Pool them using the average (across all filters and dimensions)\n",
        "pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "# Create a function that will iterate over the results.\n",
        "iterate = K.function([CBModel.input],\n",
        "[pooled_grads, last_conv_layer.get_output_at(0)[0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lz6wPHwXWrM5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we will iterate over the results. This looks for \"what exactly is activating the network\"?"
      ]
    },
    {
      "metadata": {
        "id": "K9rYxGJ1WrM6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pooled_grads_value, conv_layer_output_value = iterate([img_tensor])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uBZDr6IrWrM8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This gives the gradients, and the corresponding output, i.e. what part of the network is being activated when. We finally multiply the gradient times the values, and create a heatmap out of this."
      ]
    },
    {
      "metadata": {
        "id": "siKvU2AAWrM8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Calculate impact. We run this twice to get to a smoother heatmap.\n",
        "for j in range(2):\n",
        "    for i in range(512):\n",
        "        conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gxi88YtrWrM-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create heatmap.\n",
        "heatmap = np.mean(conv_layer_output_value, axis=-1)\n",
        "heatmap = np.maximum(heatmap, 0)\n",
        "heatmap /= np.max(heatmap)\n",
        "\n",
        "# Plot it.\n",
        "plt.matshow(heatmap)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KA45CBkrWrNA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we will overlay this map over the original picture. We will use the OpenCV package which allows for image manipulation."
      ]
    },
    {
      "metadata": {
        "id": "ZgcXbb4hWrNA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "img = cv2.imread(img_path)\n",
        "heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
        "heatmap = np.uint8(255 * heatmap)\n",
        "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "superimposed_img = cv2.addWeighted(img, 1, heatmap, 0.4, 0)\n",
        "cv2.imwrite('CBVisualized.jpg', superimposed_img)\n",
        "\n",
        "plt.imshow(superimposed_img)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x8EYX_fgWrNC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can clearly see what is happening. \n",
        "\n",
        "This is the importance of the modelling procedures: We need to understand how much diversity we are including in our images: If we don't we might end up learning other things!\n",
        "\n",
        "Try to change the filters and learning other characteristics."
      ]
    }
  ]
}
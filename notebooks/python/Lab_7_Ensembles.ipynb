{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab 7 - Ensembles and Error Measures.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CBravoR/AdvancedAnalyticsLabs/blob/master/notebooks/python/Lab_7_Ensembles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mORzER5q7ZsK"
      },
      "source": [
        "# Ensembles and Error Measures\n",
        "\n",
        "In this lab we will focus on Random Forest and XGBoosting methods created over the original data. For this, we will first import the data and re-train our original logistic regression.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHyEtC9T8VGs"
      },
      "source": [
        "## Imports and preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQnvq4b08TNp"
      },
      "source": [
        "# Import the csv files from last week.\n",
        "!gdown 'https://drive.google.com/uc?id=1LWRFLpJtTopAlRqTuUd9XZvGB6CoHa2z'\n",
        "!gdown 'https://drive.google.com/uc?id=1IvY78EGu-eizec_9agJUsQWDLT-wmSHF'\n",
        "!gdown 'https://drive.google.com/uc?id=1aDraDSR2OQbIMjIY07s-rD5cel2x_iS-'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pYwtDRG-GVo"
      },
      "source": [
        "!pip install scorecardpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kuz712WQ9rxQ"
      },
      "source": [
        "# Package loading\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scorecardpy as sc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HemZO3l8kho"
      },
      "source": [
        "# Import the files as Pandas datasets\n",
        "bankloan_train_WoE = pd.read_csv('train_woe.csv')\n",
        "bankloan_test_WoE = pd.read_csv('test_woe.csv')\n",
        "bankloan_data = pd.read_pickle('BankloanCleanNewVars.pkl')\n",
        "\n",
        "# Eliminate unused variables\n",
        "bankloan_train_WoE.drop(columns=['OthDebt_woe'], inplace = True)\n",
        "bankloan_test_WoE.drop(columns=['OthDebt_woe'], inplace = True)\n",
        "\n",
        "# Same train-test split as before (because of seed!)\n",
        "bankloan_train_noWoE, bankloan_test_noWoE = sc.split_df(bankloan_data.iloc[:, 1:],\n",
        "                                                        y = 'Default',\n",
        "                                                        ratio = 0.7,\n",
        "                                                        seed = 20190227).values()\n",
        "\n",
        "# Give breaks for WoE\n",
        "breaks_adj = {'Address': [1.0,2.0,8.0,17.0],\n",
        "              'Age': [30.0,45.0,50.0],\n",
        "              'Creddebt': [1.0, 6.0],\n",
        "              'Employ': [4.0,14.0,22.0],\n",
        "              'Income': [30.0,40.0,80.0,140.0],\n",
        "              'Leverage': [8.0,16.0,22.0],\n",
        "              'MonthlyLoad': [0.1,0.2,0.30000000000000004,0.7000000000000001],\n",
        "              'OthDebtRatio': [0.1]\n",
        "              }\n",
        "\n",
        "# Apply breaks.\n",
        "bins_adj = sc.woebin(bankloan_train_noWoE, y=\"Default\",\n",
        "                     breaks_list=breaks_adj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3diknG1AY4L"
      },
      "source": [
        "# Train logistic regression\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "\n",
        "bankloan_logreg = LogisticRegressionCV(penalty='l1', # Type of penalization l1 = lasso, l2 = ridge\n",
        "                                     Cs = 10,        # How many parameters to try. Can also be a vector with parameters to try.\n",
        "                                     tol=0.0001, # Tolerance for parameters\n",
        "                                     cv = 3,     # How many CV folds to try. 3 or 5 should be enough.\n",
        "                                     fit_intercept=True, # Use constant?\n",
        "                                     class_weight='balanced', # Weights, see below\n",
        "                                     random_state=20190301, # Random seed\n",
        "                                     max_iter=100, # Maximum iterations\n",
        "                                     verbose=0, # Show process. 1 is yes.\n",
        "                                     solver = 'saga', # How to optimize.\n",
        "                                     n_jobs = 2,      # Processes to use. Set to number of physical cores. \n",
        "                                     refit = True     # If to retrain with the best parameter and all data after finishing.\n",
        "                                    )\n",
        "\n",
        "bankloan_logreg.fit(X = bankloan_train_WoE.iloc[:, 1:], # All rows and from the second var to end\n",
        "                    y = bankloan_train_WoE['Default'] # The target\n",
        "                   )\n",
        "\n",
        "# Calculate scorecard\n",
        "bankloan_sc = sc.scorecard(bins_adj, bankloan_logreg, \n",
        "             bankloan_train_WoE.columns[1:], # The column names in the trained LR\n",
        "             points0=750, # Base points\n",
        "             odds0=0.01, # Base odds\n",
        "             pdo=50) # PDO \n",
        "\n",
        "# Applying the credit score. Applies over the original data!\n",
        "train_score = sc.scorecard_ply(bankloan_train_noWoE, bankloan_sc, \n",
        "                               print_step=0)\n",
        "test_score = sc.scorecard_ply(bankloan_test_noWoE, bankloan_sc, \n",
        "                               print_step=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fup34HDIJWwM"
      },
      "source": [
        "## Random Forests\n",
        "\n",
        "Now we will train a random forest. It is included in the ```sklearn.ensemble``` subpackage, function [```RandomForestClassifier```](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), so it is straightforward to use. It comes with many parameters, but in general there is a philosophy to follow:\n",
        "\n",
        "- In a Random Forest we want each tree to be large, and to learn as much as possible from its subset of data. We don't care too much if each tree is overadjusted, as we can always increase the number of trees to take care of this.\n",
        "\n",
        "- This said, a good idea is to limit the minimum number of samples per leaf when we have few cases (this is not usually a problem in large datasets).\n",
        "\n",
        "- We might want to limit the minimum impurity decrease to stop growing a tree if not much is happening.\n",
        "\n",
        "- There is also a class weight to include. It does include one automatically if we use the option ```balanced```.\n",
        "\n",
        "Let's train one and check the options."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHCrdH0rJgMk"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#Define the classifier\n",
        "bankloan_rf = RandomForestClassifier(n_estimators=1000, # Number of trees to train\n",
        "                       criterion='entropy', # How to train the trees. Also supports gini.\n",
        "                       max_depth=None, # Max depth of the trees. Not necessary to change.\n",
        "                       min_samples_split=2, # Minimum samples to create a split.\n",
        "                       min_samples_leaf=0.001, # Minimum samples in a leaf. Accepts fractions for %. This is 0.1% of sample.\n",
        "                       min_weight_fraction_leaf=0.0, # Same as above, but uses the class weights.\n",
        "                       max_features='auto', # Maximum number of features per split (not tree!) by default is sqrt(vars)\n",
        "                       max_leaf_nodes=None, # Maximum number of nodes.\n",
        "                       min_impurity_decrease=0.0001, # Minimum impurity decrease. This is 10^-4.\n",
        "                       bootstrap=True, # If sample with repetition. For large samples (>100.000) set to false.\n",
        "                       oob_score=True,  # If report accuracy with non-selected cases.\n",
        "                       n_jobs=2, # Parallel processing. Set to the number of cores you have. Watch your RAM!!\n",
        "                       random_state=20190305, # Seed\n",
        "                       verbose=1, # If to give info during training. Set to 0 for silent training.\n",
        "                       warm_start=False, # If train over previously trained tree.\n",
        "                       class_weight='balanced' # Balance the classes.\n",
        "                                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q4nSoXeUNPn"
      },
      "source": [
        "Now we are ready to train. We just give it our original training set variables and target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzZRHQpZUd0x"
      },
      "source": [
        "# Train the RF.\n",
        "bankloan_rf.fit(bankloan_train_noWoE.iloc[:,:-1], # X \n",
        "               bankloan_train_noWoE['Default']    # y\n",
        "                )  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouI571SgUsEs"
      },
      "source": [
        "We can see it used two jobs (two processors are available to us in this Google Colab server). It converges very quickly. Let's check how it did, this time we will print a nicer confusion matrix using seaborn, and will plot the ROC curve of the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDZAnTvYUnpD"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve\n",
        "\n",
        "# Apply the model to the test set.\n",
        "rf_pred_class_test = bankloan_rf.predict(bankloan_test_noWoE.iloc[:, :-1])\n",
        "rf_probs_test = bankloan_rf.predict_proba(bankloan_test_noWoE.iloc[:, :-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwW2hVgBVkZX"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "# Calculate confusion matrix\n",
        "confusion_matrix_rf = confusion_matrix(y_true = bankloan_test_noWoE['Default'], \n",
        "                    y_pred = rf_pred_class_test)\n",
        "\n",
        "# Turn matrix to percentages\n",
        "confusion_matrix_rf = confusion_matrix_rf.astype('float') / confusion_matrix_rf.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Turn to dataframe\n",
        "df_cm = pd.DataFrame(\n",
        "        confusion_matrix_rf, index=['good', 'bad'], columns=['good', 'bad'], \n",
        ")\n",
        "\n",
        "# Parameters of the image\n",
        "figsize = (10,7)\n",
        "fontsize=14\n",
        "\n",
        "# Create image\n",
        "fig = plt.figure(figsize=figsize)\n",
        "heatmap = sns.heatmap(df_cm, annot=True, fmt='.2f')\n",
        "\n",
        "# Make it nicer\n",
        "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, \n",
        "                             ha='right', fontsize=fontsize)\n",
        "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45,\n",
        "                             ha='right', fontsize=fontsize)\n",
        "\n",
        "# Add labels\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "\n",
        "# Plot!\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w46guL4bXh6B"
      },
      "source": [
        "Looks a bit unbalanced, but otherwise ok. It's harder to predict the  defaulters. Now let's see the ROC curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38LrSbkeVmEo"
      },
      "source": [
        "# Calculate the ROC curve points\n",
        "fpr, tpr, thresholds = roc_curve(bankloan_test_noWoE['Default'], rf_probs_test[:,1])\n",
        "\n",
        "# Save the AUC in a variable to display it. Round it first\n",
        "auc = np.round(roc_auc_score(y_true = bankloan_test_noWoE['Default'], \n",
        "                             y_score = rf_probs_test[:,1]),\n",
        "              decimals = 3)\n",
        "\n",
        "# Create and show the plot\n",
        "plt.plot(fpr,tpr,label=\"Bankloan RF, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bF-ic7GYN2g"
      },
      "source": [
        "Now, let's print the variable importance. The importance is calcualted by averaging the accuracy of trees when the variables is included the tree, and comparing it to when it's NOT included the tree."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfVlwHjk6dDt"
      },
      "source": [
        "# Plot variable importance\n",
        "importances = bankloan_rf.feature_importances_\n",
        "indices = np.argsort(importances)[::-1] \n",
        "\n",
        "f, ax = plt.subplots(figsize=(3, 8))\n",
        "plt.title(\"Variable Importance - Random Forest\")\n",
        "sns.set_color_codes(\"pastel\")\n",
        "sns.barplot(y=[bankloan_train_noWoE.iloc[:, :-1].columns[i] for i in indices], x=importances[indices], \n",
        "            label=\"Total\", color=\"b\")\n",
        "ax.set(ylabel=\"Variable\",\n",
        "       xlabel=\"Variable Importance (Entropy)\")\n",
        "sns.despine(left=True, bottom=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPP3ECvawIhT"
      },
      "source": [
        "That's it! Now we'll compare this with an XGBoost and see which one of our three models is better.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dspGHED8YVl6"
      },
      "source": [
        "## XGBoosting\n",
        "\n",
        "The stochastic gradient boosting model is the alternative to Random Forest. Now we want to create a series of small trees, which will be poorer in performance, but together they will be stronger. Training an XGBoost model is harder, because we need to control the model so it creates small trees, but it performs better in small data, something Random Forests do not necessarily accomplish.\n",
        "\n",
        "While scikit-learn does have its own implementation of XGB ([```sklearn.ensemble```](https://scikit-learn.org/stable/modules/ensemble.html)), there are a couple of very strong packages out there that implement the algorithm. ```xgboost``` and ```lightgbm``` are two of the best known ones. We will use [```xgboost```](https://xgboost.readthedocs.io/en/latest/python/) for this lab, available pretty much for every language out there.\n",
        "\n",
        "The first step is to define a classifier that we will use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2WDsujNYNBD"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "#Define the classifier.\n",
        "XGB_Bankloan = XGBClassifier(max_depth=3,                 # Depth of each tree\n",
        "                            learning_rate=0.1,            # How much to shrink error in each subsequent training. Trade-off with no. estimators.\n",
        "                            n_estimators=100,             # How many trees to use, the more the better, but decrease learning rate if many used.\n",
        "                            verbosity=1,                  # If to show more errors or not.\n",
        "                            objective='binary:logistic',  # Type of target variable.\n",
        "                            booster='gbtree',             # What to boost. Trees in this case.\n",
        "                            n_jobs=2,                     # Parallel jobs to run. Set your processor number.\n",
        "                            gamma=0.001,                  # Minimum loss reduction required to make a further partition on a leaf node of the tree. (Controls growth!)\n",
        "                            subsample=0.632,              # Subsample ratio. Can set lower\n",
        "                            colsample_bytree=1,           # Subsample ratio of columns when constructing each tree.\n",
        "                            colsample_bylevel=1,          # Subsample ratio of columns when constructing each level. 0.33 is similar to random forest.\n",
        "                            colsample_bynode=1,           # Subsample ratio of columns when constructing each split.\n",
        "                            reg_alpha=1,                  # Regularizer for first fit. alpha = 1, lambda = 0 is LASSO.\n",
        "                            reg_lambda=0,                 # Regularizer for first fit.\n",
        "                            scale_pos_weight=1,           # Balancing of positive and negative weights.\n",
        "                            base_score=0.5,               # Global bias. Set to average of the target rate.\n",
        "                            random_state=20201108,        # Seed\n",
        "                            missing=None                  # How are nulls encoded?\n",
        "                            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7NI0qGdK5Mo"
      },
      "source": [
        "This classifier can be used to tune the parameters of the model. We will use sklearn's ```GridSearchCV``` for this. It requires a dictionary of the parameters to look for. We will tune the number of trees (XGB overfits relatively easily, always tune this), the depth, and the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv93PK3hK7n9"
      },
      "source": [
        "# Define the parameters. Play with this grid!\n",
        "param_grid = dict({'n_estimators': [50, 100, 150],\n",
        "                   'max_depth': [2, 3, 4],\n",
        "                 'learning_rate' : [0.01, 0.05, 0.1, 0.15]\n",
        "                  })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jRGje4UK8vH"
      },
      "source": [
        "This training process can be very large. We will create a validation set for the sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsU6yQ5vM-rc"
      },
      "source": [
        "# Always a good idea to tune on a reduce sample of the train set, as we will call many functions.\n",
        "val_train = bankloan_train_noWoE.sample(frac = 0.5,               # The fraction to extract\n",
        "                                       random_state = 20201108    # The seed.\n",
        "                                       )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU1ZZWfrNCVt"
      },
      "source": [
        "Now we can do a grid search over the parameter space. We will use the AUC (as this is a binary classification problem)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KipB5YMwNGOg"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define grid search object.\n",
        "GridXGB = GridSearchCV(XGB_Bankloan,        # Original XGB. \n",
        "                       param_grid,          # Parameter grid\n",
        "                       cv = 3,              # Number of cross-validation folds.  \n",
        "                       scoring = 'roc_auc', # How to rank outputs.\n",
        "                       n_jobs = 2,          # Parallel jobs. -1 is \"all you have\"\n",
        "                       refit = False,       # If refit at the end with the best. We'll do it manually.\n",
        "                       verbose = 1          # If to show what it is doing.\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7OzB8G8NH2b"
      },
      "source": [
        "# Train grid search.\n",
        "GridXGB.fit(val_train.iloc[:, :-1], val_train['Default'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qbi74Z0vNT9c"
      },
      "source": [
        "Now we can output the optimal parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHDHaikMNWH1"
      },
      "source": [
        "# Show best params\n",
        "GridXGB.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lueLtcRDNZqB"
      },
      "source": [
        "It is telling us to use 1% learning rate with a max_depth of 2 and 150 trees. As the n_estimators parameter is at the limit, I would run again with a depth of four just to check one further the limit. I leave this as an exercise.\n",
        "\n",
        "Now we can fit the final model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r74qw-D7X6LG"
      },
      "source": [
        "# Create XGB with best parameters.\n",
        "XGB_Bankloan = XGBClassifier(max_depth=GridXGB.best_params_.get('max_depth'), # Depth of each tree\n",
        "                            learning_rate=GridXGB.best_params_.get('learning_rate'), # How much to shrink error in each subsequent training. Trade-off with no. estimators.\n",
        "                            n_estimators=GridXGB.best_params_.get('n_estimators'), # How many trees to use, the more the better, but decrease learning rate if many used.\n",
        "                            verbosity=1,                  # If to show more errors or not.\n",
        "                            objective='binary:logistic',  # Type of target variable.\n",
        "                            booster='gbtree',             # What to boost. Trees in this case.\n",
        "                            n_jobs=4,                     # Parallel jobs to run. Set your processor number.\n",
        "                            gamma=0.001,                  # Minimum loss reduction required to make a further partition on a leaf node of the tree. (Controls growth!)\n",
        "                            subsample=1,                  # Subsample ratio. Can set lower\n",
        "                            colsample_bytree=1,           # Subsample ratio of columns when constructing each tree.\n",
        "                            colsample_bylevel=1,          # Subsample ratio of columns when constructing each level. 0.33 is similar to random forest.\n",
        "                            colsample_bynode=1,           # Subsample ratio of columns when constructing each split.\n",
        "                            reg_alpha=1,                  # Regularizer for first fit. alpha = 1, lambda = 0 is LASSO.\n",
        "                            reg_lambda=0,                 # Regularizer for first fit.\n",
        "                            scale_pos_weight=1,           # Balancing of positive and negative weights.\n",
        "                            base_score=0.5,               # Global bias. Set to average of the target rate.\n",
        "                            random_state=20201107,        # Seed\n",
        "                            missing=None                  # How are nulls encoded?\n",
        "                            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2OLfAk9Nl5_"
      },
      "source": [
        "# Train over all training data.\n",
        "XGB_Bankloan.fit(bankloan_train_noWoE.iloc[:, :-1], bankloan_train_noWoE['Default'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwbV--RybzOl"
      },
      "source": [
        "Now we can evaluate our model. First we calculate the variable importance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XcNvyXDN3as"
      },
      "source": [
        "# Plot variable importance\n",
        "importances = XGB_Bankloan.feature_importances_\n",
        "indices = np.argsort(importances)[::-1] \n",
        "\n",
        "f, ax = plt.subplots(figsize=(3, 8))\n",
        "plt.title(\"Variable Importance - XGBoosting\")\n",
        "sns.set_color_codes(\"pastel\")\n",
        "sns.barplot(y=[bankloan_train_noWoE.iloc[:, :-1].columns[i] for i in indices], x=importances[indices], \n",
        "            label=\"Total\", color=\"b\")\n",
        "ax.set(ylabel=\"Variable\",\n",
        "       xlabel=\"Variable Importance (Entropy)\")\n",
        "sns.despine(left=True, bottom=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Z5P991nN-Xm"
      },
      "source": [
        "What do you see here? Does it make sense to you?\n",
        "\n",
        "Let's finish by plotting the evaluation measures. How does it compare to Random Forest? Why do you think this is?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ezp9txTJcP24"
      },
      "source": [
        "# Calculate probability\n",
        "XGBClassTest = XGB_Bankloan.predict(bankloan_test_noWoE.iloc[:, :-1])\n",
        "xg_probs_test = XGB_Bankloan.predict_proba(bankloan_test_noWoE.iloc[:, :-1])\n",
        "xg_probs_test = xg_probs_test[:, 1]\n",
        "\n",
        "# Calculate confusion matrix\n",
        "confusion_matrix_xgb = confusion_matrix(y_true = bankloan_test_noWoE['Default'], \n",
        "                    y_pred = XGBClassTest)\n",
        "\n",
        "# Turn matrix to percentages\n",
        "confusion_matrix_xgb = confusion_matrix_xgb.astype('float') / confusion_matrix_xgb.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Turn to dataframe\n",
        "df_cm = pd.DataFrame(\n",
        "        confusion_matrix_xgb, index=['good', 'bad'], columns=['good', 'bad'], \n",
        ")\n",
        "\n",
        "# Parameters of the image\n",
        "figsize = (10,7)\n",
        "fontsize=14\n",
        "\n",
        "# Create image\n",
        "fig = plt.figure(figsize=figsize)\n",
        "heatmap = sns.heatmap(df_cm, annot=True, fmt='.2f')\n",
        "\n",
        "# Make it nicer\n",
        "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, \n",
        "                             ha='right', fontsize=fontsize)\n",
        "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45,\n",
        "                             ha='right', fontsize=fontsize)\n",
        "\n",
        "# Add labels\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "\n",
        "# Plot!\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpyA8KBxcfFH"
      },
      "source": [
        "Very similar results. Now there is no chance to use sample weights sadly, so the solution is a bit more unbalanced. This is not too much of an issue though, as we can always change the cutoff point to account for the unbalance.\n",
        "\n",
        "Let's check the ROC curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzfLTaHmwi-R"
      },
      "source": [
        "# Calculate the ROC curve points\n",
        "fpr, tpr, thresholds = roc_curve(bankloan_test_noWoE['Default'], \n",
        "                                 xg_probs_test)\n",
        "\n",
        "# Save the AUC in a variable to display it. Round it first\n",
        "auc = np.round(roc_auc_score(y_true = bankloan_test_noWoE['Default'], \n",
        "                             y_score = xg_probs_test),\n",
        "               decimals = 3)\n",
        "\n",
        "# Create and show the plot\n",
        "plt.plot(fpr,tpr,label=\"AUC - XGBoosting = \" + str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Se_ELvsedrdy"
      },
      "source": [
        "## Plotting multiple ROC curves\n",
        "\n",
        "The last thing we would like to do is to plot multiple ROC curves in one graph. This is fairly straightforward, we just pass  the ```plt.plot``` command each of the ROC curves. I'll do it dynamically using a dictionary and a for loop. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9h7TVw_dWuJ"
      },
      "source": [
        "# Predict probabilities of scorecard.\n",
        "logreg_probs_test = bankloan_logreg.predict_proba(bankloan_test_WoE.iloc[:, 1:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RC-nLoGbfFjF"
      },
      "source": [
        "# Set models and probabilities. This structure is called a dictionary.\n",
        "models = [\n",
        "{\n",
        "    'label': 'Logistic Regression',\n",
        "    'probs': logreg_probs_test[:,1]\n",
        "},\n",
        "{\n",
        "    'label': 'Gradient Boosting',\n",
        "    'probs': xg_probs_test\n",
        "},\n",
        "{\n",
        "    'label': 'Random Forest',\n",
        "    'probs': rf_probs_test[:,1]\n",
        "}\n",
        "]\n",
        "\n",
        "# Loop that creates the plot. I will pass each ROC curve one by one.\n",
        "for m in models:\n",
        "  auc = roc_auc_score(y_true = bankloan_test_noWoE['Default'], \n",
        "                             y_score = m['probs'])\n",
        "  fpr, tpr, thresholds = roc_curve(bankloan_test_WoE['Default'], \n",
        "                                           m['probs'])\n",
        "  plt.plot(fpr, tpr, label='%s ROC (area = %0.3f)' % (m['label'], auc))\n",
        "                 \n",
        "\n",
        "    \n",
        "# Settings\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('1-Specificity(False Positive Rate)')\n",
        "plt.ylabel('Sensitivity(True Positive Rate)')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "    \n",
        "# Plot!    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7-RF4xvQfik"
      },
      "source": [
        "Interesting results, no?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCGjuwb2hW04"
      },
      "source": [
        "I introduced several new concepts here. First, a dictionary. A dictionary is a very useful structure, which allows to have values indexed by a name. Every item will have their own values for its name, here the 'label' and 'probs'. I use this as an input for the for loop.\n",
        "\n",
        "Second, check the part\n",
        "\n",
        "```\n",
        "label='%s ROC (area = %0.3f)' % (m['label'], auc)\n",
        "```\n",
        "\n",
        "of the plot definition. Note the ```%s``` and ```%0.3f``` in the string. This is a magic command that tells Python \"this is a variable, in a certain format. Go look for it outside and match it in order\". So in this case we have two:\n",
        "\n",
        "- An ```%s``` which means 'This is a string'.\n",
        "- A ```%0.3f``` which means 'This is a float (decimal), and I want it formatted using three decimals'.\n",
        "\n",
        "Outside the string there is a ```% (m['label'], auc)```. This is matched one by one, the ```%s``` to the ```m['label']``` (the string), and the ```%0.3f``` to the ```auc```. This way I can create dynamically labels. Remember to use these tricks to simplify your life! "
      ]
    }
  ]
}
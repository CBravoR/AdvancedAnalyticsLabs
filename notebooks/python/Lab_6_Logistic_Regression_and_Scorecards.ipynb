{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CBravoR/AdvancedAnalyticsLabs/blob/master/notebooks/python/Lab_6_Logistic_Regression_and_Scorecards.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IccG7h1OAa0R"
      },
      "source": [
        "# Logistic Regression and Scorecards\n",
        "\n",
        "In this lab we will finally start running models! For this we will use the excellent [```scikit-learn```](https://scikit-learn.org/stable/) package, which implements many, many data science methods. This is the go-to tool for any structured data analysis package.\n",
        "\n",
        "First, we will import the data from last week. We will download them from my Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGwyxAAuBYZ3"
      },
      "source": [
        "# Import the data files from last week.\n",
        "!gdown 'https://drive.google.com/uc?id=12AFRYPBY6N_hnvZJkSDL_nhWwjt43M-n'\n",
        "!gdown 'https://drive.google.com/uc?id=1IEvsKnMMwHrOsqR1EaaaMQcms1vTiQgu'\n",
        "!gdown 'https://drive.google.com/uc?id=1aDraDSR2OQbIMjIY07s-rD5cel2x_iS-'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGdEWvmjBaWo"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEzFvUnCPlqs"
      },
      "source": [
        "Now we install the scorecardpy package and clean our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6leC6-rRPrs7"
      },
      "source": [
        "!pip install git+https://github.com/CBravoR/scorecardpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr3dMH2OS3X0"
      },
      "source": [
        "# Data wrangling\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "import polars.selectors as cs\n",
        "\n",
        "# Scorecard construction\n",
        "import scorecardpy as sc\n",
        "import numpy as np\n",
        "\n",
        "# Sampling from scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# Formatting\n",
        "from string import ascii_letters\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, SGDClassifier\n",
        "from sklearn.metrics import confusion_matrix, log_loss, auc, roc_auc_score, roc_curve\n",
        "from sklearn.utils.class_weight import compute_class_weight\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp0no9a1Ps4S"
      },
      "source": [
        "# Import the files\n",
        "bankloan_train_WoE = pl.read_parquet('train_woe.parquet')\n",
        "bankloan_test_WoE = pl.read_parquet('test_woe.parquet')\n",
        "bankloan_data = pd.read_pickle('BankloanCleanNewVars.pkl')\n",
        "\n",
        "# Eliminate unused variables\n",
        "# bankloan_data.drop(columns=['Education'], inplace = True)\n",
        "\n",
        "# Same train-test split as before (because of seed!)\n",
        "bankloan_train_noWoE, bankloan_test_noWoE = train_test_split(bankloan_data, # Dataframe\n",
        "                                                             test_size=0.3, # Test percentage\n",
        "                                                             random_state=20251023, # Seed for reproducibility\n",
        "                                                             stratify=bankloan_data['Default'] # How to stratify sampling\n",
        "                                                             )\n",
        "\n",
        "# Give breaks for WoE\n",
        "breaks_adj = {'Address': [1.0,2.0,7.0,11.0],\n",
        "              'Age': [21.0,30.0,37.0,46.0],\n",
        "              'Creddebt': [1.0,6.0],\n",
        "              'Education': ['Bas','Posg%,%SupInc','Med','SupCom'],\n",
        "              'Employ': [2.0,4.0,11.0,18.0],\n",
        "              'Income': [30.0,40.0,80.0,140.0],\n",
        "              'Leverage': [3.0,7.0,10.0,17.0],\n",
        "              'MonthlyLoad': [0.1,0.25,0.65],\n",
        "              'OthDebt': [0.4, 1.6, 3.2],\n",
        "              'OthDebtRatio': [0.04,0.07,0.09,0.13]\n",
        "              }\n",
        "\n",
        "# Apply breaks.\n",
        "bins_adj = sc.woebin(bankloan_train_noWoE, y=\"Default\",\n",
        "                     breaks_list=breaks_adj)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxHzupctVYi3"
      },
      "source": [
        "## Generating a logistic regression object\n",
        "\n",
        "To train a logistic regression, we first need to create an object that stores how we want the model to be trained. In general, all of scikit-learn models work this way:\n",
        "\n",
        "- We create the model we want to train, with all required parameters. This model is **not trained yet**, it just keeps the logic we will use.\n",
        "\n",
        "- We apply the ```fit``` function to the object we just created. This takes as input the training set and the targets (if the model is supervised), and will update our model with trained parameters.\n",
        "\n",
        "- We then used our trained model to apply it to a test set, and calculate outputs.\n",
        "\n",
        "Logistic regression is included in the [```linear_model subpackage```](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model) and it comes pre-packaged with all regularization algorithms: the LASSO penalization, the Ridge penalization and the ElasticNet method (refer to the lectures for the explanation of these, or read this [excellent tutorial](https://codingstartups.com/practical-machine-learning-ridge-regression-vs-lasso/)).\n",
        "\n",
        "In a nutshell, LASSO and Ridge are going to penalize including variables by adding either a linear (LASSO) or quadratic (Ridge) term to the minimization algorithm, or a combination of the two if using Elastic Net.\n",
        "\n",
        "These methods have hypermparameters that need to be optimized. For this we will use a cross-validation procedure (again, refer to the lectures). Luckily for us, scikit-learn already comes with an object that will allow cross-validated optimization of the penalization parameter. The function to call is[```LogisticRegressionCV```](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV)\n",
        "\n",
        "Let's start by creating this object.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB56nPSfVU5E"
      },
      "source": [
        "bankloan_logreg = LogisticRegressionCV(penalty='elasticnet', # Type of penalization l1 = lasso, l2 = ridge, elasticnet\n",
        "                                     Cs = 10,        # How many parameters to try. Can also be a vector with parameters to try.\n",
        "                                     tol=0.000001, # Tolerance for parameters\n",
        "                                     cv = 3,     # How many CV folds to try. 3 or 5 should be enough.\n",
        "                                     fit_intercept=True, # Use constant?\n",
        "                                     class_weight='balanced', # Weights, see below\n",
        "                                     random_state=20190301, # Random seed\n",
        "                                     max_iter=100, # Maximum iterations\n",
        "                                     verbose=2, # Show process. 1 is yes.\n",
        "                                     solver='saga', # How to optimize.\n",
        "                                     n_jobs=2,      # Processes to use. Set to number of physical cores.\n",
        "                                     refit=True,     # If to retrain with the best parameter and all data after finishing.\n",
        "                                     l1_ratios = np.arange(0, 1.01, 0.1), # The LASSO / Ridge ratios.\n",
        "                                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weIQIKZEaP6o"
      },
      "source": [
        "Let's dig deeper into what is needed.\n",
        "\n",
        "**Penalty**\n",
        "\n",
        "'l1' penalty refers to LASSO regression (great at selecting variables), 'l2' to Ridge regression (not very good at selecting variables), and 'elasticnet'. My advice: As long as you have more samples than variables, start with LASSO, if it doesn't work or you are not happy with the results, move to elasticnet.\n",
        "\n",
        "**Penalty constants to try (```Cs```)**\n",
        "\n",
        "This refers to how many LASSO or Ridge parameters to try. These parameters measure the weight of the error in prediction versus the regularization (penalty) error. When optimizing the parameters, a penalization constant will try to optimise the following:\n",
        "\n",
        "$$\n",
        "Error = Error_{prediction} + \\frac{1}{C} \\times Error_{penalty}\n",
        "$$\n",
        "\n",
        "So the $C$ constant will balance both objectives. By giving a Cs larger than 1, it will try as many parameters as given.\n",
        "\n",
        "**Class weighting**\n",
        "\n",
        "Most interesting problems are unbalanced. This means the interesting class (Default in our case) has less cases than the opposite class. Models optimise the sum over **all** cases, so if we minimize the error, which class do you think will be better classified?\n",
        "\n",
        "This means we need to balance the classes to make them equal. Luckily for us, Scikit-Learn includes automatic weighting that assigns the same error to both classes. The error becomes the following:\n",
        "\n",
        "$$\n",
        "Error = Weight_1 \\times Error_{predictionClass1} + Weight_2 \\times Error_{predictionClass2} + \\frac{1}{C} \\times Error_{penalty}\n",
        "$$\n",
        "\n",
        "The weights are selected so the theoretical maximum error in both classes is the same (see the help for the exact equation).\n",
        "\n",
        "**Random State**\n",
        "\n",
        "The random seed. Remember to use your student ID.\n",
        "\n",
        "**Iterations**\n",
        "\n",
        "The solution comes from an iterative model, thus we specify a maximum number of iterations. Remember to check for convergence after it has been solved!\n",
        "\n",
        "**Solver**\n",
        "\n",
        "Data science functions are complex ones, with thousands, millions, or even billions of parameters. Thus we need to use the best possible solver for our problems. Several are implemented in scikit-learn. The help states that:\n",
        "\n",
        "\n",
        "- For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones.\n",
        "- For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss; ‘liblinear’ is limited to one-versus-rest schemes.\n",
        "- ‘newton-cg’, ‘lbfgs’ and ‘sag’ only handle L2 penalty, whereas ‘liblinear’ and ‘saga’ handle L1 penalty.\n",
        "\n",
        "We will use 'saga', a very efficient solver. You can read all about it [here](https://www.di.ens.fr/~fbach/Defazio_NIPS2014.pdf).\n",
        "\n",
        "**refit**\n",
        "\n",
        "If your data is sufficiently small to fit in memory, you will be able to use all of the training data for the cross-validation process. If so, then with ```refit=True``` you will retrain the model after the parameter search, using the optimal parameter found.\n",
        "\n",
        "However, in large datasets this might not be possible. In this case:\n",
        "\n",
        "1. Obtain a **validation sample** from the original training data. Usually 20% of data is used, but it depends on memory and time constraints.\n",
        "\n",
        "2. Run the Cross-validation process over this validation data and find the optimal parameter. Let's call it $C^*$.\n",
        "\n",
        "3. Train a logistic regression with all training data, but with a fixed parameter $C^*$. For this you need to use the function [```LogisticRegression```](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) in scikit-learn and give the parameter ```C=YOUR_OPTIMAL_C```. The rest of the parameters are similar to ```LogisticRegressionCV```.\n",
        "\n",
        "The ```LogisticRegression``` object has another interesting interesting parameter for big data models. ```warm_start```.\n",
        "\n",
        "**Warm start**\n",
        "\n",
        "Scikit-learn allows for multiple adjustments to the training. For example, you can try first with a little bit of data just to check if everything is working, and then, if you set ```warm_start = True``` before, it will retrain starting from the original parameters. Allows for dynamic updating as well.  ```warm_start = False``` means whenever we give it new data, it will start from scratch, forgetting what it previously learned.\n",
        "\n",
        "**l1_ratios**\n",
        "\n",
        "These are the balance parameters between LASSO and Ridge for the ElasticNet optimization, with 0 <= l1_ratio <= 1. A value of 0 is equivalent to using penalty='l2', while 1 is equivalent to using penalty='l1'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDmULPDedbKx"
      },
      "source": [
        "## Training!\n",
        "\n",
        "Now we are ready to train. We simply apply the method ```fit``` to our data, giving it the training set and the target variable as inputs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bankloan_train_WoE.columns"
      ],
      "metadata": {
        "id": "2tZTn7OMprvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGzK11UOdUDE"
      },
      "source": [
        "bankloan_logreg.fit(X=bankloan_train_WoE.drop(\"Default\"),\n",
        "                    y=bankloan_train_WoE['Default'] # The target\n",
        "                   )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpaX6Nz5fS31"
      },
      "source": [
        "Let's read the output:\n",
        "\n",
        "```convergence after 25 epochs took 0 seconds```\n",
        "\n",
        "The method was able to find a solution at the given tolerance, and it took 16 iterations and almost no time. **If the method says it did not converge then you need to increase iterations, change C or both!**.\n",
        "\n",
        "The rest of the output refers to what it did, it is not relevant at this stage.\n",
        "\n",
        "Done! We have a logistic regression! Let's check the parameters, sorted into a nice table.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7Y-z7naeZk9"
      },
      "source": [
        "coef_df = pd.concat([pd.DataFrame({'column': bankloan_train_WoE.drop(\"Default\").columns}),\n",
        "                    pd.DataFrame(np.transpose(bankloan_logreg.coef_))],\n",
        "                    axis = 1\n",
        "                   )\n",
        "\n",
        "coef_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51iSCJE6gPo0"
      },
      "source": [
        "We can see the parameter for each variable now. This does not include the constant. We can get it with"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpNZLhOOfytV"
      },
      "source": [
        "bankloan_logreg.intercept_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4HYfdaXnJPk"
      },
      "source": [
        "We can see all variables are being used, and the intercept is really close to 0. This is expected in a balanced logistic regression that uses WoE transform and is a way to check everything is working as intended.\n",
        "\n",
        "We can see that most coefficients are correctly determined, even in the presence of correlations. This happens because the **ElasticNet penalty deals with correlations gracefully**. This is NOT the case if we had a LASSO regression. Try it yourself and see. In that case, you would need to manually eliminate the variables so everything works correctly.\n",
        "\n",
        "However, Income does have a coefficient that is correlated with others. At this point, it may be better to remove the variable and rerun our models."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrain, without income.\n",
        "x_drop = [\"Default\", \"Income_woe\"]\n",
        "bankloan_logreg.fit(X = bankloan_train_WoE.drop(x_drop),\n",
        "                    y = bankloan_train_WoE['Default'] # The target\n",
        "                   )\n",
        "\n",
        "# Check coefficients.\n",
        "coef_df = pd.concat([pd.DataFrame({'column': bankloan_train_WoE.drop(x_drop).columns}),\n",
        "                    pd.DataFrame(np.transpose(bankloan_logreg.coef_))],\n",
        "                    axis = 1\n",
        "                   )\n",
        "\n",
        "coef_df"
      ],
      "metadata": {
        "id": "b5dhWvDtcUC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19LLh3CzGEyP"
      },
      "source": [
        "We can also check the optimal hyperparameters found."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LS6iDSe0M3Fy"
      },
      "source": [
        "print(f\"The L1 ratio is {bankloan_logreg.l1_ratio_.item():.3f}\")\n",
        "print(f\"The C parameter is {bankloan_logreg.C_.item():.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we can train the final logistic regression."
      ],
      "metadata": {
        "id": "d4y4xuuB2TEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the object\n",
        "bankloan_logreg = LogisticRegression(penalty='elasticnet', # Type of penalization l1 = lasso, l2 = ridge, elasticnet\n",
        "                                     C = 0.359,        # How many parameters to try. Can also be a vector with parameters to try.\n",
        "                                     l1_ratio = 0.100, # l1_ratio\n",
        "                                     tol=0.000001, # Tolerance for parameters\n",
        "                                     fit_intercept=True, # Use constant?\n",
        "                                     class_weight='balanced', # Weights, see below\n",
        "                                     random_state=20190301, # Random seed\n",
        "                                     max_iter=100, # Maximum iterations\n",
        "                                     verbose=2, # Show process. 1 is yes.\n",
        "                                     solver = 'saga', # How to optimize.\n",
        "                                     n_jobs = 2,      # Processes to use. Set to number of physical cores.\n",
        "                                    )\n",
        "\n",
        "# Train it\n",
        "bankloan_logreg.fit(X = bankloan_train_WoE.drop(x_drop),\n",
        "                    y = bankloan_train_WoE['Default'])"
      ],
      "metadata": {
        "id": "cxKXEYyu2SnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyhGIFcfn1sO"
      },
      "source": [
        "## Applying to the test set\n",
        "\n",
        "We can now apply our results to the test set, and check our results. Most models in scikit-learn have the ```predict``` method which applies the model to new data, this gives the 0-1 prediction. Alternatively (and more usefully) we can use the ```predict_proba``` method that gives the probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8ckdDCGntCt"
      },
      "source": [
        "pred_class_test = bankloan_logreg.predict(bankloan_test_WoE.drop(x_drop))\n",
        "probs_test = bankloan_logreg.predict_proba(bankloan_test_WoE.drop(x_drop))\n",
        "print(probs_test[0:5], pred_class_test[0:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3BPKP7Zope6"
      },
      "source": [
        "Scikit-learn will give, by default, one probability per class.  The second column is the one that applies for class Default = 1.\n",
        "\n",
        "We will get the confusion matrix to check our accuracy. These are included in the subpackage ```sklearn.metrics``` and we will plot it using seaborn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seZjBboipd6G"
      },
      "source": [
        "# Calculate confusion matrix\n",
        "confusion_matrix_cs = confusion_matrix(y_true = bankloan_test_WoE['Default'],\n",
        "                                        y_pred = pred_class_test)\n",
        "\n",
        "\n",
        "# Turn matrix to percentages\n",
        "confusion_matrix_cs = confusion_matrix_cs.astype('float') / confusion_matrix_cs.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Turn to dataframe\n",
        "df_cm = pd.DataFrame(\n",
        "        confusion_matrix_cs, index=['Non Defaulter', 'Defaulter'],\n",
        "        columns=['Non Defaulter', 'Defaulter'],\n",
        ")\n",
        "\n",
        "# Parameters of the image\n",
        "figsize = (10,7)\n",
        "fontsize=14\n",
        "\n",
        "# Create image\n",
        "fig = plt.figure(figsize=figsize)\n",
        "heatmap = sns.heatmap(df_cm, annot=True, fmt='.2f')\n",
        "\n",
        "# Make it nicer\n",
        "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0,\n",
        "                             ha='right', fontsize=fontsize)\n",
        "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45,\n",
        "                             ha='right', fontsize=fontsize)\n",
        "\n",
        "# Add labels\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "\n",
        "# Plot!\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwMDtGwdq35F"
      },
      "source": [
        "Pretty good model!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Larger-than-memory training: Partial fit.\n",
        "\n",
        "What if our data does not fit in memory? In these cases, most models come with some sort of solution. Polars uses the very useful [LazyFrame](https://docs.pola.rs/api/python/stable/reference/lazyframe/index.html) which can delay reading the data until the very latest. But that has the problem that we would still need to be able to read the full data in chunks. How can we train the model over partial segments of data so we can train even if our data does not fit in RAM?\n",
        "\n",
        "Combining LazyFrames with the models that, in ```sklearn```, have a [```partial_fit```](https://scikit-learn.org/0.15/modules/scaling_strategies.html#incremental-learning) method available, this can be easily done. This is called **Incremental Learning**. Besides many sklearn methods, XGBoost also comes with this functionality, as we will see in future labs.\n",
        "\n",
        "There is one important consideration to have in this type of fit: You must pass the full classes on the very first ```partial_fit``` so that the model understand all the classes that are available. Other than that, partial data is handled gracefully.\n",
        "\n",
        "For logistic regression, the classifier we will use is the Stochastic Gradient Descent classifier [```SGDClassifier```](https://scikit-learn.org/0.15/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier). The strategy will be:\n",
        "\n",
        "1. Get the dimension of the data using polars. We will now not load the data until the very last possible moment.\n",
        "2. Load a chunk that we know fits in memory. Our data is small, so for the purpose of our example, we will train in chunks of 200 cases.\n",
        "3. Iterate until we cover the full data. For that, randomly select 200 cases of the train set, pass it to ```partial_fit``` and check convergence. We stop once the model stops training.\n",
        "4. Validate the model over the test set and compare our performance.\n",
        "\n",
        "Let's start by creating our LazyFrame and checking the dimension."
      ],
      "metadata": {
        "id": "rhzzuXg5eCQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bankloan_lazy_train = pl.scan_parquet('train_woe.parquet')\n",
        "bankloan_lazy_train.describe()"
      ],
      "metadata": {
        "id": "li5f_cJag-zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have the same 1034 cases as before. This operation was run **without ever loading the full dataset into memory**. We can, in theory, run descriptives over thousands of variables as long as one fits in memory.\n",
        "\n",
        "Let's assume we measured the RAM usage and we determined that we can load at most **200 cases** into memory. How can we train over those? For that, we first need to define an SGD classifier."
      ],
      "metadata": {
        "id": "dQ-8EU1ThNG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a random column for sampling.\n",
        "bankloan_lazy_train = bankloan_lazy_train.with_columns(\n",
        "    pl.lit(np.random.rand(bankloan_lazy_train.collect().height)).alias(\"rand\")\n",
        "    )\n",
        "\n",
        "# Split train and validation.\n",
        "bankloan_lazy_train_sample = bankloan_lazy_train.filter(pl.col(\"rand\") < 0.7).drop(\"Income_woe\")\n",
        "bankloan_lazy_val_sample = bankloan_lazy_train.filter(pl.col(\"rand\") >= 0.7).drop(\"Income_woe\").collect()\n",
        "\n",
        "# Describe train\n",
        "bankloan_lazy_train_sample.describe()\n",
        "\n",
        "# Calculate the class weight\n",
        "class_weight = compute_class_weight(\"balanced\", classes=np.array([0,1]),\n",
        "                                    y=bankloan_lazy_train_sample.select(pl.col(\"Default\")).collect().to_numpy().ravel()\n",
        "                                    )\n",
        "\n",
        "# Alternative: never upload default to memory.\n",
        "# pos_cases = bankloan_lazy_train_sample.select(pl.col(\"Default\")).sum().collect().to_numpy().ravel().item()\n",
        "# neg_cases = bankloan_lazy_train_sample.select(pl.col(\"Default\")).count().collect().to_numpy().ravel().item() - pos_cases\n",
        "# class_weight = {0: 1, 1:  neg_cases / pos_cases}\n",
        "# print(class_weight)\n",
        "\n",
        "# Calculate class_weight dictionary\n",
        "class_weight = {0: class_weight[0], 1: class_weight[1]}\n",
        "\n",
        "# Define SGDClassifier\n",
        "sgd_bankloan = SGDClassifier(loss='log_loss',  # Loss to use. Logistic regression equals 'log'\n",
        "                             penalty='elasticnet', # Type of penalization l1 = lasso, l2 = ridge, elasticnet\n",
        "                             alpha=0.359, #Multiplicative constant of the penalization.\n",
        "                             l1_ratio=0.100, # Lasso penalty\n",
        "                             verbose=0, # Output to give. Write 0 for silent training.\n",
        "                             n_jobs=-1, # Use all cores.\n",
        "                             random_state=20251030, # Random state.\n",
        "                             class_weight=class_weight, # How to balance classes.\n",
        "                             warm_start=False # If each call to fit deletes the original parameters. VERY IMPORTANT IN INCREMENTAL LEARNING.\n",
        "                             )"
      ],
      "metadata": {
        "id": "a9WANBmyhnud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the model defined, we can split the train and validation set."
      ],
      "metadata": {
        "id": "zfJ0CaJ2i6O5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our training sample has 726 cases. We will select around 200 per iteration, that is approximately 30% of the dataset. Let's now create the loop. We will run 100 iterations, or until the error does not decrease beyond 0.1%."
      ],
      "metadata": {
        "id": "CMgd9JMaoMS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the arrays where we will save the performance.\n",
        "train_loss = np.array([])\n",
        "\n",
        "# Define the constants to measure.\n",
        "best_loss = 10e9\n",
        "n_iter = 1000\n",
        "batch_fraction = 0.33\n",
        "tol = 10e-6\n",
        "\n",
        "# Create the loop\n",
        "for i in range(n_iter):\n",
        "  # Get the sample from the train set. Select 33% of total.\n",
        "  train_slice = bankloan_lazy_train_sample.filter(\n",
        "    pl.linear_space(0, 1, pl.len()) # Generate random number\n",
        "      .sample(fraction=1, with_replacement=True, shuffle=True) <= batch_fraction).drop(\"rand\") # Sample\n",
        "\n",
        "  # Collect to RAM\n",
        "  train_slice = train_slice.collect()\n",
        "\n",
        "  # Train the chunk\n",
        "  sgd_bankloan.partial_fit(X = train_slice.drop(\"Default\"),\n",
        "                           y = train_slice[\"Default\"],\n",
        "                           classes=np.array([0, 1]))\n",
        "\n",
        "  # Get the values of the loss over the validation set.\n",
        "  y_pred_proba = sgd_bankloan.predict_proba(bankloan_lazy_val_sample.drop([\"Default\", \"rand\"]))\n",
        "  iter_log_loss = log_loss(bankloan_lazy_val_sample[\"Default\"], y_pred_proba)\n",
        "  train_loss = np.append(train_loss, iter_log_loss)\n",
        "  tol_iter = np.abs(best_loss - iter_log_loss)\n",
        "  print(f\"Iteration {i}: Log Loss = {iter_log_loss:.3f}, Tolerance = {tol_iter:.6f}\")\n",
        "\n",
        "  # Check if best_loss is worse than current loss. If so, replace.\n",
        "  if best_loss > iter_log_loss:\n",
        "    best_loss = iter_log_loss\n",
        "\n",
        "  # Check if tolerance is within range.\n",
        "  if tol_iter < tol:\n",
        "    print(\"Converged!\")\n",
        "    break\n"
      ],
      "metadata": {
        "id": "V_72trBHoGdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have a model fully trained!"
      ],
      "metadata": {
        "id": "27z3m6NQw1vm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check coefficients.\n",
        "coef_df = pd.concat([pd.DataFrame({'column': bankloan_lazy_val_sample.drop([\"Default\", \"rand\"]).columns}),\n",
        "                    pd.DataFrame(np.transpose(sgd_bankloan.coef_))],\n",
        "                    axis = 1\n",
        "                   )\n",
        "\n",
        "coef_df"
      ],
      "metadata": {
        "id": "DjjBrXNFIWpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smaB3Mq3OKAk"
      },
      "source": [
        "## Scorecards\n",
        "\n",
        "The package ```scorecardpy``` has the function ```scorecard``` which receives a trained logistic regression model trained over WoE-transformed data, a trained scorecard **over the same variables** and a list of matched columns (that is, the order of columns in the scorecard). As optional arguments it receives a PDO, a base score, and decimal base odds (so instead of 50:1, it receives 0.02).\n",
        "\n",
        "You should adjust these values so the score is in a range that's acceptable. Typically between 0 and 1000."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCipEb4CO-RJ"
      },
      "source": [
        "bankloan_sc = sc.scorecard(bins_adj,         # bins from the WoE\n",
        "                           bankloan_logreg,  # Trained logistic regression\n",
        "                           bankloan_test_WoE.drop(x_drop).columns, # The column names in the trained LR\n",
        "                           points0=750, # Base points\n",
        "                           odds0=0.01, # Base odds bads:goods\n",
        "                           pdo=50\n",
        "                           ) # PDO\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bankloan_sc"
      ],
      "metadata": {
        "id": "Ynpyss8x28HV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2z72hOjuPD1p"
      },
      "source": [
        "# Applying the credit score. Applies over the original data!\n",
        "train_score = sc.scorecard_ply(bankloan_train_noWoE, bankloan_sc,\n",
        "                               print_step=0)\n",
        "test_score = sc.scorecard_ply(bankloan_test_noWoE, bankloan_sc,\n",
        "                               print_step=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XxmR_1SPIBP"
      },
      "source": [
        "train_score.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ROC Curves\n",
        "\n",
        "To finish the lab, let's compare the logistic regression and the SGD over the test set."
      ],
      "metadata": {
        "id": "tuaohF_0w72z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predicted probabilities over the test set\n",
        "y_pred_proba_test = bankloan_logreg.predict_proba(bankloan_test_WoE.drop(x_drop))\n",
        "y_pred_proba_test_sgd = sgd_bankloan.predict_proba(bankloan_test_WoE.drop([\"Income_woe\",\"Default\"]))\n",
        "\n",
        "# Set models and probabilities. This structure is called a dictionary.\n",
        "models = [\n",
        "{\n",
        "    'label': 'Logistic Regression',\n",
        "    'probs': y_pred_proba_test[:,1]\n",
        "},\n",
        "{\n",
        "    'label': 'SGD',\n",
        "    'probs': y_pred_proba_test_sgd[:,1]\n",
        "}\n",
        "]\n",
        "\n",
        "# Loop that creates the plot. I will pass each ROC curve one by one.\n",
        "for m in models:\n",
        "  auc = roc_auc_score(y_true = bankloan_test_WoE['Default'],\n",
        "                             y_score = m['probs'])\n",
        "  fpr, tpr, thresholds = roc_curve(bankloan_test_WoE['Default'],\n",
        "                                           m['probs'])\n",
        "  plt.plot(fpr, tpr, label=f'{m[\"label\"]} ROC (area = {auc:.3f})')\n",
        "\n",
        "\n",
        "# Settings\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('1-Specificity(False Positive Rate)')\n",
        "plt.ylabel('Sensitivity(True Positive Rate)')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "# Plot!\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-Z7uuGeMxDA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see a slight difference between the SGD training and the LogReg training, due to the training strategy. Still, pretty close!"
      ],
      "metadata": {
        "id": "sLSGPUWIyyAy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftudtXeaPMEI"
      },
      "source": [
        "And that's it! We have a fully functional credit scorecard. In later labs we will contrast this with two more models: a Random Forest and an XGBoost model."
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CBravoR/AdvancedAnalyticsLabs/blob/master/notebooks/python/Lab_PD_Calibration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcxUDe3dWTOx"
      },
      "source": [
        "# PD Calibration\n",
        "\n",
        "In this lab we will learn how to estimate the long-run PD after a model has been trained. The PD calibration can be done with the score, the monthly portfolio each case belongs to (usually the behavioural scorecard is used), the labels (Default / Non-Default) and a set of economic factors. For this work we will use an exchange rate and a commodity price.\n",
        "\n",
        "First, we load the data. It is in Excel, so we use the appropriate function from Pandas. There are two worksheets: The first one contains the data for each borrower and each portfolio, and the second one contains the macro factor at each month."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastexcel\n",
        "!pip install pwlf"
      ],
      "metadata": {
        "id": "leQ4-Gc57eRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ti0EPEhUaUBL"
      },
      "source": [
        "# Base and data reading\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# Transformation\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Curve cutting\n",
        "import pwlf\n",
        "\n",
        "# Bigger and prettier plots\n",
        "plt.rcParams['figure.figsize'] = (10, 5)\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "# Date and time\n",
        "from datetime import date\n",
        "from dateutil.relativedelta import *\n",
        "\n",
        "# Time series\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "\n",
        "# Normal function from scipy\n",
        "from scipy.stats import norm\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4aDtYQiacvR"
      },
      "source": [
        "# Download Excel file\n",
        "!gdown 'https://drive.google.com/uc?id=1UYmgsu5gI5U_VbraKXHxWTXyZSbM6q5S'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsBV0xEuaodx"
      },
      "source": [
        "# Load the data. Two datasets are necessary.\n",
        "loans = pl.read_excel('PDCalExample.xlsx', # Filename\n",
        "                      sheet_id=1,         # Worksheet name or index\n",
        "                      )\n",
        "\n",
        "econ_factors = pl.read_excel('PDCalExample.xlsx', # Filename\n",
        "                             sheet_id=2,         # Worksheet name or index\n",
        "                             )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "econ_factors.head()"
      ],
      "metadata": {
        "id": "YN02Gw5nz3NZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loans.head()"
      ],
      "metadata": {
        "id": "Mq1LUS9Ozpq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f\"There are {loans.select(\"Portfolio\").unique().count().to_numpy().item()} unique monthly portfolios\""
      ],
      "metadata": {
        "id": "9_fAxkK80AAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WguMyC__bqJ6"
      },
      "source": [
        "loans.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12vzlarlbtCp"
      },
      "source": [
        "econ_factors.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qz3eQS9Ci1s"
      },
      "source": [
        "Let's normalize the economic factors using a z-transform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNjL6BHPCmjO"
      },
      "source": [
        "# Define the transformer and set the output to Polars\n",
        "transformer = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"scaler\", StandardScaler(), [\"Commodity\", \"ExchangeRate\"])\n",
        "    ],\n",
        "    remainder='passthrough',\n",
        "    verbose_feature_names_out=False\n",
        ")\n",
        "transformer.set_output(transform=\"polars\")\n",
        "\n",
        "# Fit and transform the data\n",
        "econ_factors = transformer.fit_transform(econ_factors)\n",
        "\n",
        "# Check the output\n",
        "econ_factors.head()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1Tz_odabxtx"
      },
      "source": [
        "## Defining Ratings\n",
        "\n",
        "We have all the data we need. Let's start then by obtaining PD segments. Basel suggests building 7-15 segments. For this, we can use the excellent package [pwlf](https://pypi.org/project/pwlf/). It will allow segmenting a curve using a given number of cuts.\n",
        "\n",
        "Which curve do we need to segment? The ROC curve!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wgbSkL5bugp"
      },
      "source": [
        "# Calculate the ROC curve points\n",
        "fpr, tpr, thresholds = roc_curve(loans['Default'],\n",
        "                                 loans['Probs'])\n",
        "\n",
        "# Save the AUC in a variable to display it. Round it first\n",
        "auc = np.round(roc_auc_score(y_true = loans['Default'],\n",
        "                             y_score = loans['Probs']),\n",
        "               decimals = 3)\n",
        "\n",
        "# Create and show the plot\n",
        "plt.plot(fpr,tpr,label=\"Scorecard, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ash0iqXQdMzB"
      },
      "source": [
        "Now we can segment the curve. The process takes a while to run, and sadly it is sequential, so go make yourself a coffee / tea while this runs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjCu3fRUcr3i"
      },
      "source": [
        "# Define the curve with the ROC curve\n",
        "piecewise_AUC = pwlf.PiecewiseLinFit(fpr, tpr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "516fa5czzdsv"
      },
      "source": [
        "# Calculate the best curve. Long!\n",
        "res = piecewise_AUC.fit(10, disp=True, seed=20251127)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxSDJS5Jde1f"
      },
      "source": [
        "As this is a long process, it is a good idea to save the results. The object res can be pickled, or simply the cuts can be saved in a commented line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qws6gZyZc0Lb"
      },
      "source": [
        "res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZG91rd0dolJ"
      },
      "source": [
        "# Use previous result\n",
        "# res = [0.        , 0.01383389, 0.03920524, 0.07731607, 0.11285577,\n",
        "#      0.20653665, 0.32339978, 0.41818781, 0.57283343, 0.7999917 ,\n",
        "#      1.\n",
        "#      ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e05QF34ywlIg"
      },
      "source": [
        "ROC_curve = pd.DataFrame({'fpr': fpr, 'threshold': thresholds})\n",
        "ROC_curve"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Q2M5LWmdyBW"
      },
      "source": [
        "To apply the cuts, you can use the method ```fit_with_breaks``` that is available for the ```piecewise_AUC``` object.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3oCTTJUd1BD"
      },
      "source": [
        "# Apply cuts!\n",
        "piecewise_AUC.fit_with_breaks(res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMUtYqPIeS7X"
      },
      "source": [
        "We can now apply this to our dataset and see how the piecewise curve fits the ROC curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTVL2JHQeQ18"
      },
      "source": [
        "# predict for the determined points\n",
        "xHat = np.linspace(min(fpr), max(fpr), num=10000)\n",
        "yHat = piecewise_AUC.predict(xHat)\n",
        "\n",
        "# plot the results\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, 'o')\n",
        "plt.plot(xHat, yHat, '-')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaXnfklSefXv"
      },
      "source": [
        "We have a pretty good fit! But we are still not done, we now need to understand if the cuts lead to monotonic PDs. This is the final constraint. For this, we first calculate the PD for the whole dataset (we will adjust this later for each portfolio)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res"
      ],
      "metadata": {
        "id": "zMwdfrpJEUJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMm6-9Sux9Eq"
      },
      "source": [
        "# Find probability associated with every cut\n",
        "pbb_cuts = np.zeros_like(res)\n",
        "i = 0\n",
        "\n",
        "# Find the threshold value that correspond to the closest point in the ROC_curve object's fpr, to each element of the res object.\n",
        "for element in res:\n",
        "  pbb_cuts[i] = ROC_curve.loc[ROC_curve['fpr'].sub(element).abs().idxmin(), 'threshold']\n",
        "  i += 1\n",
        "\n",
        "# Reverse the vector\n",
        "pbb_cuts = np.flip(pbb_cuts)\n",
        "\n",
        "# Show the outcome\n",
        "pbb_cuts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K85KdeOzzHTN"
      },
      "source": [
        "# Insert 0 in the first position, and replace the last value to one.\n",
        "pbb_cuts = np.insert(pbb_cuts, 0, 0)\n",
        "pbb_cuts[-1] = 1\n",
        "pbb_cuts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUSBgjTced-E"
      },
      "source": [
        "# Segment the probabilities\n",
        "pd_cut = pd.cut(loans['Probs'], pbb_cuts)\n",
        "pd_cut"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVScSagae_-F"
      },
      "source": [
        "And we study the output with a crosstab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c4PZHO7e6OM"
      },
      "source": [
        "# Create table with cases total.\n",
        "PDs_Tab = pd.crosstab(pd_cut,\n",
        "                      loans['Default'],\n",
        "                      normalize = False)\n",
        "\n",
        "# Calculate default rate.\n",
        "print(PDs_Tab)\n",
        "pd_final = PDs_Tab[1] / (PDs_Tab[0] + PDs_Tab[1])\n",
        "pd_final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T31FLRergAro"
      },
      "source": [
        "The PD is perfectly monotonous! We should however combine the two cuts, as they contain no defaulters. We may want to tweak this result so that we obtain sufficiently representative PDs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjusted cuts. Drop the second element in pbb_cuts\n",
        "pbb_cuts = np.delete(pbb_cuts, 1)"
      ],
      "metadata": {
        "id": "JFWmRab5J7gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "After doing this, our cuts are reasonable and we can now proceed to calculate the PDs for every portfolio. For this, we need to calculate the average number of defaults for each portfolio, over the total number of cases that month."
      ],
      "metadata": {
        "id": "8btuE3pEJ9dL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrzXIrMLhV6I"
      },
      "source": [
        "# Cut probabilities\n",
        "pd_cut = pd.cut(loans['Probs'], pbb_cuts).astype('str')\n",
        "\n",
        "# Add the PDCut variable to our Polars dataframe. Give it name 'PD_Cut'.\n",
        "loans = loans.with_columns(\n",
        "    pl.Series(values=pd_cut, name='PD_Cut')\n",
        "    )\n",
        "\n",
        "# Create pivot table\n",
        "PD_monthly = loans.pivot(values = 'Default',\n",
        "                         index = 'Portfolio',\n",
        "                         on='PD_Cut',\n",
        "                         aggregate_function='mean'\n",
        "                         )\n",
        "\n",
        "PD_monthly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO-jn7pzkixX"
      },
      "source": [
        "Now we have calculated the PDs for all ratings! We ended up with exactly the Basel recommendation. On larger portfolios you will easily reach the 7-15 range.\n",
        "\n",
        "Let's plot how our ratings look like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmZCROhHjzCZ"
      },
      "source": [
        "PD_monthly.drop(\"Portfolio\").to_pandas().plot(subplots=True,\n",
        "          layout = (3,4),\n",
        "          sharex=False,\n",
        "          sharey=False,\n",
        "          colormap='viridis',\n",
        "         fontsize=1,\n",
        "         legend=False,\n",
        "         linewidth=0.2);\n",
        "plt.tight_layout();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCKEJ53r04T3"
      },
      "source": [
        "We see that some months have no defaulters. With the very limited data we have that is to be expected, but in large portfolios this will be softer. In general all time series look stationary.\n",
        "\n",
        "And that's almost it! Now we are ready to calibrate these PDs across the multiple cuts for the long-term or for IFRS 9."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF0XEfsK1fK0"
      },
      "source": [
        "## Estimating Long-Term PD Using Vasicek\n",
        "\n",
        "To estimate the long-term PD, we need to estimate what the Z factor is given the factor and the macroeconomic factors. For this, we can use the subpackage [Time Series Analysis](https://www.statsmodels.org/stable/tsa.html#module-statsmodels.tsa) (```tsa```) of the ```statsmodel``` package. We aim to run a SARIMAX model, so we should study stationary properties, seasonality, etc.\n",
        "\n",
        "We begin by estimating the Z factor from our data. The equation is:\n",
        "\n",
        "$$\n",
        "Z_t = \\frac{\\Phi^{-1}(PD_{TTC}) - \\sqrt{1-\\rho}\\Phi^{-1}(DR_t)}{\\sqrt{\\rho}}\n",
        "$$\n",
        "\n",
        "where $\\rho$ comes from either Basel or the asset correlation to the economy as measured by the company and $DR_t$ is the observed default rate (in our case, each element of ```PD_monthly```. The $PD_{TTC}$ is the average PD for the rating."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the parameters. I will assume a fixed mortgage rho for this case (0.15). Adapt for your portfolio!\n",
        "rho = 0.15\n",
        "\n",
        "# Convert PD_monthly Polars DataFrame to Pandas DataFrame for compatibility with apply and norm.ppf\n",
        "PD_monthly_pd = PD_monthly.to_pandas().fillna(0)\n",
        "\n",
        "# Calculating the average PD per rating, from PD_monthly_pd, calculated as the average per column.\n",
        "# Drop the 'Portfolio' column (if it exists as a column after to_pandas) before calculating the mean across columns (axis=0)\n",
        "PD_monthly_avg = PD_monthly_pd.drop(\"Portfolio\", axis=1).mean(axis=0)\n",
        "print(PD_monthly_avg)\n",
        "\n",
        "# Create an empty Z_t dataframe. The index should be the 'Portfolio' values.\n",
        "# The columns should be the rating categories.\n",
        "# Note: PD_monthly_pd will have a default integer index (0 to 107) and 'Portfolio' as a regular column.\n",
        "Z_t = pd.DataFrame(index=PD_monthly_pd['Portfolio'], columns=PD_monthly_pd.columns.drop('Portfolio'))\n",
        "\n",
        "# Populate Z_t using the Vasicek equation.\n",
        "# Iterate through each rating category (column name) in Z_t\n",
        "for rating_col in Z_t.columns:\n",
        "    # Get the monthly default rates for the current rating category\n",
        "    dr_t = PD_monthly_pd[rating_col]\n",
        "\n",
        "    # Get the average PD for the current rating category\n",
        "    pd_ttc = PD_monthly_avg[rating_col]\n",
        "\n",
        "    # Adjust dr_t values to avoid -inf or inf from norm.ppf(0) or norm.ppf(1)\n",
        "    # Replace 0 with a very small number (e.g., 1e-6) and 1 with a number slightly less than 1 (e.g., 1 - 1e-6)\n",
        "    dr_t_adjusted = dr_t.apply(lambda x: 1e-6 if x == 0 else (1 - 1e-6 if x == 1 else x))\n",
        "\n",
        "    # Calculate the numerator of the Vasicek equation\n",
        "    # norm.ppf(pd_ttc) is used directly as pd_ttc values are already between (0,1)\n",
        "    numerator = norm.ppf(pd_ttc) - np.sqrt(1-rho) * norm.ppf(dr_t_adjusted)\n",
        "\n",
        "    # Calculate Z_t for the current rating column\n",
        "    Z_t[rating_col] = numerator / np.sqrt(rho)\n",
        "\n",
        "# Z_t dataframe now contains the calculated Z_t values\n",
        "Z_t"
      ],
      "metadata": {
        "id": "-0bUeN3mNAQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKaz7tlY5mWG"
      },
      "source": [
        "Let's construct a time series from the Z factors. The first step is to turn the Portfolio index into a TimeSeries index. This way Python knows it is dealing with dates. Our data starts in January 1999 and it is monthly data. We arbitrarily set our first day as the 1st of January 1999 and add up from there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtCoqv-z6Q9C"
      },
      "source": [
        "start_date = date(1999, 1, 1)\n",
        "\n",
        "# Transform econ factors to pandas and add index\n",
        "econ_factors = econ_factors.to_pandas()\n",
        "econ_factors.index = [pd.to_datetime(start_date + relativedelta(months=+portfolio_month-1)) for portfolio_month in econ_factors.index]\n",
        "econ_factors.drop(columns='Portfolio', inplace = True)\n",
        "\n",
        "# Add index to Z_t\n",
        "Z_t.index = [pd.to_datetime(start_date + relativedelta(months=+portfolio_month-1)) for portfolio_month in PD_monthly_pd.index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Z_t"
      ],
      "metadata": {
        "id": "vIc7aCiKVC0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8Rp1sl09nfE"
      },
      "source": [
        "Now we can decompose our time series to see what is happening.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNL3_ZPblz6z"
      },
      "source": [
        "decomposition = seasonal_decompose(Z_t.iloc[:, 4], model='additive')\n",
        "fig = decomposition.plot()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3jsmbEh9-3O"
      },
      "source": [
        "The time series look fairly stationary, with yearly seasonality. We are read to estimate a model!\n",
        "\n",
        "Within the ```tsa``` package, the [SARIMAX model](https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html#statsmodels.tsa.statespace.sarimax.SARIMAX), present in the subpackage ```statespace```, is the most general model available, allowing for Seasonality, Auto Regression, Integration, Moving Averages, and eXogenous factors. We will train a simpler ARX model (autoregressive with exogenous regressors), but you are welcome to experiment further, more complex, regressions.\n",
        "\n",
        "Let's search for the best model for a rating, searching between 1 and 6 autoregression factors, using the macroeconomic factors as the exogenous variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puNin-aY4Mxy"
      },
      "source": [
        "# Define the search space.\n",
        "p = range(1, 6)\n",
        "d = range(0, 2)\n",
        "q = range(0, 2)\n",
        "\n",
        "# Create an interative list of ps, ds, qs.\n",
        "pdq = list(product(p, d, q))\n",
        "\n",
        "# Seasonal parameters. One year back.\n",
        "ps = range(0, 4)\n",
        "ds = range(0, 1)\n",
        "qs = range(0, 1)\n",
        "seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(product(ps, ds, qs))]\n",
        "\n",
        "# Train the models for a series and test multiple values.\n",
        "y = Z_t.iloc[:, 4] # Choose the fifth rating\n",
        "\n",
        "\n",
        "auc_out = []\n",
        "\n",
        "for param in pdq:\n",
        "  for param_seasonal in seasonal_pdq:\n",
        "      mod = SARIMAX(y,\n",
        "                    exog=np.asarray(econ_factors),\n",
        "                    order=param,\n",
        "                    seasonal_order=param_seasonal,\n",
        "                    enforce_stationarity=False,\n",
        "                    enforce_invertibility=False\n",
        "                    )\n",
        "      results = mod.fit()\n",
        "      auc_out.append([param, param_seasonal, results.aic])\n",
        "      print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))\n",
        "\n",
        "# Nicer formatting\n",
        "auc_out = pd.DataFrame(auc_out,\n",
        "                       columns = ['(p,q,r)', '(ps, qs, rs, S)', 'AIC'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5PYdY1qDS_U"
      },
      "source": [
        "The warning is due to the frequency of the time series which is not provided by Pandas. You can safely ignore it.\n",
        "\n",
        "Let's see which one is the resulting model. AIC should be minimized."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auc_out.sort_values(by='AIC', ascending=True)"
      ],
      "metadata": {
        "id": "kNIV7YBtZPCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We can see from these values that the prefer model with parameters (5, 1, 1) \tand (3, 0, 0, 12).\n",
        "\n",
        "Let's test the final model for these values."
      ],
      "metadata": {
        "id": "QFaGRYqWZ16t"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoXxxcPICHwW"
      },
      "source": [
        "# ARIMA(5, 1, 1) \t(3, 0, 0, 12)\n",
        "mod_BB = SARIMAX(y,\n",
        "              exog=np.asarray(econ_factors),\n",
        "              order=(5, 1, 1),\n",
        "              seasonal_order=(3, 0, 0, 12),\n",
        "              enforce_stationarity=False,\n",
        "              enforce_invertibility=False)\n",
        "results_BB = mod_BB.fit()\n",
        "\n",
        "print(results_BB.summary().tables[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yog1AfBWEV-R"
      },
      "source": [
        "The results show what, for this dataset, the first coefficient is useful, while the second is not. On the lags and autorregressive parameters, most are not significant. We should try a simple regression of the Z_t factors on the macro only, maybe that will give better results.\n",
        "\n",
        "It is not perfect, so we should be a bit careful with this model and possibly study other structures. I leave this as an exercise!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now recover our calibrated PD by inverting the process, using the estimated Z_t for any values of the economic factors we want.\n",
        "\n",
        "- For **Basel** use the long-run estimates of the economy.\n",
        "- For **IFRS 9** use the current values (or the predicted ones) for the current month.\n",
        "\n",
        "You can calculate these PDs with the Vasicek equation.\n",
        "\n",
        "$$\n",
        "PD_{PIT} = \\Phi \\left(\\frac{\\Phi^{-1}(PD_{TTC}) - \\sqrt{\\rho}Z_t}{\\sqrt{1-\\rho}}\\right)\n",
        "$$"
      ],
      "metadata": {
        "id": "GB1bWAaHVcrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Average Economic Factors\n",
        "average_econ_factors = econ_factors.mean().to_frame().T\n",
        "print(\"Average Economic Factors:\")\n",
        "print(average_econ_factors)\n",
        "\n",
        "# Predict Z-factor for Average Economic Conditions\n",
        "# The model 'results_BB' was trained on Z_t.iloc[:, 4], which corresponds to the rating '(0.0, 0.222]'\n",
        "predicted_z_factor = results_BB.predict(start=len(econ_factors), end=len(econ_factors), exog=average_econ_factors)\n",
        "predicted_z_factor_value = predicted_z_factor.iloc[0]\n",
        "print(\"\\nPredicted Z-factor for average economic conditions (for the fifth rating):\")\n",
        "print(predicted_z_factor_value)\n",
        "\n",
        "# Get the average TTC PD for the fifth rating used for training the model\n",
        "pd_ttc_fifth_rating = PD_monthly_avg.iloc[4] # Corresponds to '(0.0, 0.222]'\n",
        "print(\"\\nAverage TTC PD for the fifth rating ('(0.0, 0.222]'):\")\n",
        "print(pd_ttc_fifth_rating)\n",
        "\n",
        "# Calculate Calibrated PD (PD_PIT) using the inverse Vasicek formula\n",
        "calibrated_pd_pit = norm.cdf( (norm.ppf(pd_ttc_fifth_rating) - np.sqrt(rho) * predicted_z_factor_value) / np.sqrt(1 - rho) )\n",
        "\n",
        "print(\"\\nCalibrated Point-in-Time (PIT) PD for the fifth rating under average economic conditions:\")\n",
        "print(calibrated_pd_pit)\n"
      ],
      "metadata": {
        "id": "j8PxHqvfVXvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the calibrated PIT PD (8.656e-07) compared to the TTC PD (0.00707) under average economic conditions is much lower. This suggests that the current average economic environment, as captured by the Z-factor, is considerably benign for this category.\n",
        "\n",
        "And that's it! Repeating this process for every time series leads to our calibrated PDs. Now what we would do is to use the long-run estimates for the economic factors to reach a long-term PD. An alternative approach is to just run **one** regression, using the rating as a dummy predictor variable (potentially interacting with the econ factors). This is simpler, but may not have the best results. Try both approaches!\n",
        "\n",
        "For LGD, the process is the same. The only difference is that you need to use the **downturn** economic factors instead of the long-run economic factors"
      ],
      "metadata": {
        "id": "NJlxziEkVgOe"
      }
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab - 2D Convolutions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CBravoR/AdvancedAnalyticsLabs/blob/master/notebooks/python/Lab_2D_Convolutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KefNczLWrL8"
      },
      "source": [
        "# 2D Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neKsl_mxWrL9"
      },
      "source": [
        "In this lab, we will train a traditional VGG16 model, where we will train the last couple of layers to adapt it to our problem.\n",
        "\n",
        "The problem is to predict one of 6 categories of pictures, whether they are. 'buildings', 'forest', 'glacier', 'mountain', 'sea' or 'street'. This problem comes from an Intel Image Classification challenge and is available [here](https://www.kaggle.com/puneet6060/intel-image-classification/version/2).\n",
        "\n",
        "First, we load the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLUGY4T_W4Z1"
      },
      "source": [
        "!gdown 'https://drive.google.com/uc?export=download&id=1HEB7JHl6uSANiENvaHBxhzmkkZ9YlhEC'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha2b0IUlXibC"
      },
      "source": [
        "!unzip IntelClassification.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYYauoNEWrL-"
      },
      "source": [
        "## VGG16\n",
        "\n",
        "The VGG 16 model is a classic model in Deep Learning. It is a 16 layer model, following the structure that we discussed in the lectures.\n",
        "\n",
        "This model was trained over the ImageNet data, thus looking to classify among 1000 different types of objects, over a very large database of images. We can leverage these already-trained weights, and  adapt just the last few layers for our purposes.\n",
        "\n",
        "We start by loading the VGG16 model. Keras comes pre-packaged with a series of models, loaded into the [Applications library](https://keras.io/applications/). We start by first loading the model on-the-fly using the library. We can check the options of the model in the options of the function [VGG16](https://keras.io/applications/#vgg16).\n",
        "\n",
        "We also need a package that allows for an efficient storage of the model using a binary format. The package is called [h5py](https://www.h5py.org/) and also allows for storing your pre-trained models. You can read a tutorial for this [here](https://machinelearningmastery.com/save-load-keras-deep-learning-models/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vD3pJQRDWrMB"
      },
      "source": [
        "import numpy as np\n",
        "import h5py as h5py\n",
        "import PIL\n",
        "\n",
        "# Others\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# For AUC estimation and ROC plots\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Plots\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el4B7dJ_WrMD"
      },
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "model = VGG16(weights = 'imagenet',      # The weights from the ImageNet competition\n",
        "              include_top = False,       # Do not include the top layer, which classifies.\n",
        "              input_shape= (224, 224, 3) # Input shape. Three channels, and BGR (NOT RGB!!!)\n",
        "             )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iaFjronWrMH"
      },
      "source": [
        "This will download the model and save it to our unoriginally named variable model. It is a pretty beefy download, at around 500MB. Luckly we have fast internet in the lab :)\n",
        "\n",
        "We can now check the details of the model as usual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "dzEtNZi0WrMH"
      },
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "from IPython.display import Image\n",
        "\n",
        "plot_model(model, show_shapes=True, show_layer_names=True, to_file='GraphModel.png')\n",
        "Image(retina=True, filename='GraphModel.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDVzqhVBWrMJ"
      },
      "source": [
        "At this point, every single parameter is trainable. We don't need this, as we want to use the parameters that come with the model. We will create a parallel model to store the new trainable layers, and then set all of these layers as untrainable. We will finally add a Dense layer with 128 neurons, plus a Dense layer with two classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wn6IyJsGWrMK"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJLpTDWzWrMM"
      },
      "source": [
        "# Create new model\n",
        "CBModel = Sequential()\n",
        "\n",
        "# Copy the layers to our new model. This needs to be done as there is a bug in Keras.\n",
        "for layer in model.layers:\n",
        "    CBModel.add(layer)\n",
        "\n",
        "# Set the layers as untrainable\n",
        "for layer in CBModel.layers:\n",
        "    layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j37nOTAzUxcr"
      },
      "source": [
        "CBModel.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut5Wlab-wtXG"
      },
      "source": [
        "# Set layer as trainable.\n",
        "CBModel.layers[15].trainable = True\n",
        "CBModel.layers[16].trainable = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVqY193_WrMP"
      },
      "source": [
        "# We now add the new layers for prediction.\n",
        "CBModel.add(Flatten(input_shape=model.output_shape[1:]))\n",
        "CBModel.add(Dense(64, activation = 'relu'))\n",
        "CBModel.add(Dropout(0.5))\n",
        "CBModel.add(Dense(64, activation = 'relu'))\n",
        "CBModel.add(Dropout(0.5))\n",
        "CBModel.add(Dense(6, activation = 'softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3jEAWLcWrMR"
      },
      "source": [
        "# How does the model look like?\n",
        "CBModel.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9I2NBj-WWrMT"
      },
      "source": [
        "# Compiling the model!\n",
        "CBModel.compile(loss='categorical_crossentropy', # This is NOT a binary problem!\n",
        "              optimizer=optimizers.SGD(learning_rate=0.001),\n",
        "              metrics=['accuracy']\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94fkn6CvWrMV"
      },
      "source": [
        "Ready! We can now load the data. We have our set of pictures ready for this example. For this problem we will use a generator. A generator takes images from a directory, and feeds them to the model as needed. **This is necessary to work with big data**. We cannot expect the datasets we work here to fit in memory, so we take the images as needed.\n",
        "\n",
        "We will first build two image generators (one for testing and one for training), which will generate new samples on the fly using our pictures as input.\n",
        "\n",
        "We will also conduct **data augmentation**, which are a series of mathematical operations over the datasets to make them search more complex patterns. If you use augmentation, learning will take longer but be more robust. The process to work with this data is the following:\n",
        "\n",
        "1. Create an [ImageDataGenerator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) object which will process the images and load them as needed.\n",
        "\n",
        "2. Call the ```flow_from_directory``` from our generator which will split the data into two parts, one for training and one for validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c4AnfLGWrMW"
      },
      "source": [
        "# prepare data augmentation configuration. One for train, one for test.\n",
        "train_datagen = ImageDataGenerator(\n",
        "                                  rescale=1. / 255,                         # NNets like small inputs. Rescale.\n",
        "                                  shear_range=0.2,                          # Shear?\n",
        "                                  zoom_range=0.2,                           # Zoom? 0.2 means from 80% to 120%\n",
        "                                  horizontal_flip=True,                     # Flip horizontally?\n",
        "                                  vertical_flip=False,                      # Flip vertically?\n",
        "                                  preprocessing_function=preprocess_input,  # VGG expects specific input. Set it up with this function that comes prepackaged.\n",
        "                                  validation_split = 0.2                    # Create a validation cut?\n",
        "                                  )\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "                                  rescale=1. / 255,                       # NNets like small inputs. Rescale.\n",
        "                                  shear_range=0,                          # Shear?\n",
        "                                  zoom_range=0,                           # Zoom? 0.2 means from 80% to 120%\n",
        "                                  horizontal_flip=False,                  # Flip horizontally?\n",
        "                                  vertical_flip=False,                    # Flip vertically?\n",
        "                                  preprocessing_function=preprocess_input,# VGG expects specific input. Set it up with this function that comes prepackaged.\n",
        "                                  validation_split = 0                    # No validation cut for test.\n",
        "                                  )\n",
        "\n",
        "\n",
        "# We will use a batch size of 64. Depends on RAM of GPU.\n",
        "batch_size = 64\n",
        "\n",
        "# Train data generator. We point to the training directory!\n",
        "train_data_dir = 'IntelClassification/seg_train'\n",
        "\n",
        "# VGG requires 224 x 224 images.\n",
        "(img_height, img_width) = (224, 224)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "                                                    train_data_dir,                      # Where are the pics\n",
        "                                                    target_size=(img_height, img_width), # What size should they be\n",
        "                                                    batch_size=batch_size,               # Size of batch\n",
        "                                                    class_mode='categorical',            # Class mode, whether 'binary' or 'categorical'\n",
        "                                                    subset = 'training',                 # What subset to use?\n",
        "                                                    shuffle = True                       # Shuffle the data?\n",
        "                                                    )\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "                                                    train_data_dir,                      # Where are the pics\n",
        "                                                    target_size=(img_height, img_width), # What size should they be\n",
        "                                                    batch_size=batch_size,               # Size of batch\n",
        "                                                    class_mode='categorical',            # Class mode, whether 'binary' or 'categorical'\n",
        "                                                    subset = 'validation',               # What subset to use?\n",
        "                                                    shuffle = True                       # Shuffle the data?\n",
        "                                                    )\n",
        "\n",
        "# Test data generator.\n",
        "test_data_dir = 'IntelClassification/seg_test'\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "                                                  test_data_dir,\n",
        "                                                  target_size=(img_height, img_width),\n",
        "                                                  batch_size=1,\n",
        "                                                  class_mode='categorical',\n",
        "                                                  shuffle = False\n",
        "                                                  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELxVqH7IWrMY"
      },
      "source": [
        "Now the system is ready to train from the images that we have loaded. We now feed the generators to the model, and ask to train for a certain number of epochs. We found the following datasets:\n",
        "\n",
        "- Train: 11,230 images belonging to 6 classes.\n",
        "- Validation: Found 2804 images belonging to 6 classes.\n",
        "- Test: 3000 images belonging to 6 classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk9tyeE1WrMY"
      },
      "source": [
        "# Number of epochs\n",
        "epochs = 10\n",
        "\n",
        "# Train!\n",
        "CBModel.fit(\n",
        "            train_generator,\n",
        "            epochs=epochs,\n",
        "            validation_data=validation_generator,\n",
        "            steps_per_epoch = 32, # Usually cases / batch_size = 176. Reduced to 32 so it runs faster.\n",
        "            validation_steps = 44 # Number of validation steps. Again cases / batch_size = 44.\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFOBLbK9WrMa"
      },
      "source": [
        "The model is able to learn quite well! Let's check the convergence plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kD5qzVeBvEsK"
      },
      "source": [
        "loss = CBModel.history.history['loss']\n",
        "val_loss = CBModel.history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxwL7VUQMSrh"
      },
      "source": [
        "As you can see, the model still has much more to learn! Keep training until you get convergence.\n",
        "\n",
        "Now, let's apply it to the test set and get a confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bah8rC0sbSGt"
      },
      "source": [
        "# Applying to the test set with a generator.\n",
        "test_generator.reset()\n",
        "\n",
        "# Get probabilities\n",
        "output = CBModel.predict(test_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZK_Pi0PfYoF"
      },
      "source": [
        "# Calculate classes\n",
        "pred_classes = np.argmax(output, axis=1)\n",
        "\n",
        "labels = test_generator.classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOYmieUDqWoG"
      },
      "source": [
        "np.which(labels, "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiWE90q1dEOq"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "\n",
        "# Calculate confusion matrix\n",
        "confusion_matrix_net = confusion_matrix(y_true = labels, \n",
        "                    y_pred = pred_classes)\n",
        "\n",
        "# Turn matrix to percentages\n",
        "confusion_matrix_net = confusion_matrix_net.astype('float') / confusion_matrix_net.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Turn to dataframe\n",
        "df_cm = pd.DataFrame(\n",
        "        confusion_matrix_net, \n",
        "        index=['building', 'forest', 'glacier', 'mountain', 'sea', 'street'],\n",
        "        columns=['building', 'forest', 'glacier', 'mountain', 'sea', 'street'], \n",
        ")\n",
        "\n",
        "# Parameters of the image\n",
        "figsize = (10,7)\n",
        "fontsize=14\n",
        "\n",
        "# Create image\n",
        "fig = plt.figure(figsize=figsize)\n",
        "heatmap = sns.heatmap(df_cm, annot=True, fmt='.2f')\n",
        "\n",
        "# Make it nicer\n",
        "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, \n",
        "                             ha='right', fontsize=fontsize)\n",
        "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45,\n",
        "                             ha='right', fontsize=fontsize)\n",
        "\n",
        "# Add labels\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "\n",
        "# Plot!\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnI4UU5Mj8Ro"
      },
      "source": [
        "Now that we are happy, we can save the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw8bU1OfkG2z"
      },
      "source": [
        "# Activating Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IP9KxzTCj7Mb"
      },
      "source": [
        "# Saving the model\n",
        "CBModel.save('/content/drive/MyDrive/VGG16_FM9528A.h5')\n",
        "\n",
        "# Loading\n",
        "# CBModel = keras.models.load_model('/content/drive/MyDrive/VGG16_FM9528A.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKk9lwInm4wl"
      },
      "source": [
        "/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBQ9y9kUWrMr"
      },
      "source": [
        "## Visualizing learning\n",
        "\n",
        "As a final example. We will visualize the learning, to detect exactly what is happening.\n",
        "\n",
        "We will use [Grad CAM](https://arxiv.org/abs/1610.02391), a method that allows visualizing how one image activates the neural network. Basically we will look for the direction that the model used to get to its decisions. This requires to do some iterations that we won't explain in detail. Please refer to chapter 5.4 of Chollet's book to check the details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWB2fu7si0YF"
      },
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Display\n",
        "from IPython.display import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETHLyIIaavPm"
      },
      "source": [
        "# Get the image in the right size\n",
        "def get_img_array(img_path, size = (224, 224)):\n",
        "    import tensorflow as tf\n",
        "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=size)\n",
        "    # `array` is a float32 Numpy array of shape (299, 299, 3)\n",
        "    array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    # We add a dimension to transform our array into a \"batch\"\n",
        "    # of size (1, 224, 224, 3)\n",
        "    array = np.expand_dims(array, axis=0)\n",
        "    array = preprocess_input(array)\n",
        "    return array\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHJ4oJkldXa2"
      },
      "source": [
        "# Get an image\n",
        "img_path = 'IntelClassification/seg_test/buildings/20057.jpg'\n",
        "data = get_img_array(img_path)\n",
        "\n",
        "# Plot it\n",
        "display(Image(img_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDAuzsAGemCi"
      },
      "source": [
        "# The explainer. Gotten from https://keras.io/examples/vision/grad_cam/\n",
        "def make_gradcam_heatmap(\n",
        "    img_array, model, last_conv_layer_name, classifier_layer_names\n",
        "):\n",
        "    from tensorflow import keras\n",
        "    import tensorflow as tf\n",
        "    # First, we create a model that maps the input image to the activations\n",
        "    # of the last conv layer\n",
        "    last_conv_layer = model.get_layer(last_conv_layer_name)\n",
        "    last_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output)\n",
        "\n",
        "    # Second, we create a model that maps the activations of the last conv\n",
        "    # layer to the final class predictions\n",
        "    classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\n",
        "    x = classifier_input\n",
        "    for layer_name in classifier_layer_names:\n",
        "        x = model.get_layer(layer_name)(x)\n",
        "    classifier_model = keras.Model(classifier_input, x)\n",
        "\n",
        "    # Then, we compute the gradient of the top predicted class for our input image\n",
        "    # with respect to the activations of the last conv layer\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Compute activations of the last conv layer and make the tape watch it\n",
        "        last_conv_layer_output = last_conv_layer_model(img_array)\n",
        "        tape.watch(last_conv_layer_output)\n",
        "        # Compute class predictions\n",
        "        preds = classifier_model(last_conv_layer_output)\n",
        "        top_pred_index = tf.argmax(preds[0])\n",
        "        top_class_channel = preds[:, top_pred_index]\n",
        "\n",
        "    # This is the gradient of the top predicted class with regard to\n",
        "    # the output feature map of the last conv layer\n",
        "    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n",
        "\n",
        "    # This is a vector where each entry is the mean intensity of the gradient\n",
        "    # over a specific feature map channel\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    # We multiply each channel in the feature map array\n",
        "    # by \"how important this channel is\" with regard to the top predicted class\n",
        "    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n",
        "    pooled_grads = pooled_grads.numpy()\n",
        "    for i in range(pooled_grads.shape[-1]):\n",
        "        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n",
        "\n",
        "    # The channel-wise mean of the resulting feature map\n",
        "    # is our heatmap of class activation\n",
        "    heatmap = np.mean(last_conv_layer_output, axis=-1)\n",
        "\n",
        "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
        "    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n",
        "    return heatmap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnGJfyiTesO1"
      },
      "source": [
        "# Print the predictions\n",
        "preds = CBModel.predict(data)\n",
        "print(preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXyf-H42jjav"
      },
      "source": [
        "Now we need to configure our model. We neeed to get the layers that are used for prediction (i.e. everything after the last convolution) and the last convolutional layer.\n",
        "\n",
        "Get them with a model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhkDA-Waj0Ky"
      },
      "source": [
        "CBModel.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAN1gRKKfRMm"
      },
      "source": [
        "# Set the layers.\n",
        "last_conv_layer_name = \"block5_conv3\"\n",
        "classifier_layer_names =  [\"block5_pool\", \n",
        "                           \"flatten_1\",\n",
        "                           \"dense_3\",\n",
        "                           \"dense_4\",\n",
        "                           \"dense_5\",]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYhNzl2yj6QB"
      },
      "source": [
        "# Plot the heatmap!\n",
        "heatmap = make_gradcam_heatmap(\n",
        "    data, CBModel, last_conv_layer_name, classifier_layer_names\n",
        ")\n",
        "\n",
        "# Display heatmap\n",
        "plt.matshow(heatmap)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQeGF5W2j8lK"
      },
      "source": [
        "Now, to really visualize what's going on, we will superimpose the heatmap over the input. The following code does just that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTTWChq9iXcW"
      },
      "source": [
        "# We load the original image\n",
        "img = keras.preprocessing.image.load_img(img_path)\n",
        "img = keras.preprocessing.image.img_to_array(img)\n",
        "\n",
        "# We rescale heatmap to a range 0-255\n",
        "heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "# We use jet colormap to colorize heatmap\n",
        "jet = cm.get_cmap(\"jet\")\n",
        "\n",
        "# We use RGB values of the colormap\n",
        "jet_colors = jet(np.arange(256))[:, :3]\n",
        "jet_heatmap = jet_colors[heatmap]\n",
        "\n",
        "# We create an image with RGB colorized heatmap\n",
        "jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n",
        "jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
        "jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n",
        "\n",
        "# Superimpose the heatmap on original image\n",
        "superimposed_img = jet_heatmap * 0.4 + img\n",
        "superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n",
        "\n",
        "# Save the superimposed image\n",
        "save_path = \"Building_Example.jpg\"\n",
        "superimposed_img.save(save_path)\n",
        "\n",
        "# Display Grad CAM\n",
        "display(Image(save_path))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8EYX_fgWrNC"
      },
      "source": [
        "Now we can clearly see what is happening. \n",
        "\n",
        "This is the importance of the modelling procedures: We need to understand how much diversity we are including in our images: If we don't we might end up learning other things!\n",
        "\n",
        "Try to change the filters and learning other characteristics."
      ]
    }
  ]
}
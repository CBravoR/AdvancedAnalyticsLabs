{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab - LGD Modelling.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOPlKjgtU7QjaiNJHLXIQcs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CBravoR/AdvancedAnalyticsLabs/blob/master/notebooks/python/Lab_LGD_Modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCdDMmB_v44r"
      },
      "source": [
        "# LGD Modelling\r\n",
        "\r\n",
        "In this lab, we will model the LGD using two techniques: A linear regression, a a fitted distribution regression, and a random forest. LGD models are particularly tricky as they tend to have oddly-shaped distributions, thus traditional methods do not tend to create good fit for the models.\r\n",
        "\r\n",
        "First, we will load and study the data.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHDwxH75v9Uy"
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=1nldxUFNGDziLZgE-fJv5KmNjnbdM29na"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxDyuCDMwU_L"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY0FqCqBzKTX"
      },
      "source": [
        "LGD_data = pd.read_csv('LGD.csv')\r\n",
        "LGD_data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYkfGBd13SjV"
      },
      "source": [
        "Let's create a test / train split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGmz9JwdzixC"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "x_train, x_test, y_train, y_LGD_test = train_test_split( LGD_data.iloc[:, 0:13], # Predictors\r\n",
        "                                                    LGD_data['LGD'],         # Target variable\r\n",
        "                                                    test_size=0.33,          # Test size percentage\r\n",
        "                                                    random_state=20201209    # Seed\r\n",
        "                                                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKwHZthd3Rj_"
      },
      "source": [
        "And finally let's plot the LGD distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-PWh6cf3ePp"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "sns.set_theme(style=\"darkgrid\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvYbqyUM3jkU"
      },
      "source": [
        "# Create the figure with the density\r\n",
        "fig = sns.displot(LGD_data['LGD'], kind = 'kde')\r\n",
        "\r\n",
        "# Create a density histogram\r\n",
        "sns.histplot(LGD_data['LGD'], stat = 'density')\r\n",
        "\r\n",
        "# Plot the whole thing\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pSmK4FX4A_h"
      },
      "source": [
        "As we can see, the LGD has an unbalanced bimodal distribution between 0 and 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUnsKjEn4m-7"
      },
      "source": [
        "## Linear regression\r\n",
        "\r\n",
        "We will now try to fit a basic linear regression and see its performance. For this we will use the linear regression implementation of scikit-learn, [```linear_model```](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model). We will also regularize using ElasticNet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1BXc0Ne4ws4"
      },
      "source": [
        "# Importing the package.\r\n",
        "from sklearn.linear_model import ElasticNetCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39HgtUAi6mAJ"
      },
      "source": [
        "LGD_linear_model = ElasticNetCV(l1_ratio=np.arange(0.01, 1.01, 0.05),  # l1_ratios to try.\r\n",
        "                                n_alphas=10,                        # How many alphas to try per l1_ratio\r\n",
        "                                fit_intercept=True,                 # Use constant?\r\n",
        "                                max_iter=1000,                      # Iterations\r\n",
        "                                tol=0.0001,                         # Parameter tolerance\r\n",
        "                                cv=3,                               # Number of cross_validation folds\r\n",
        "                                verbose=True,                       # Explicit or silent training\r\n",
        "                                n_jobs=2,                           # Cores to use\r\n",
        "                                random_state=20201209               # Random seed\r\n",
        "                                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzygFoI09Kg8"
      },
      "source": [
        "LGD_linear_model.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIP3m8f_9iUE"
      },
      "source": [
        "Let's check the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WgcXjW_9X71"
      },
      "source": [
        "coef_df = pd.concat([pd.DataFrame({'column': x_train.columns}), \r\n",
        "                    pd.DataFrame(np.transpose(LGD_linear_model.coef_))],\r\n",
        "                    axis = 1\r\n",
        "                   )\r\n",
        "\r\n",
        "coef_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxaF2PEe9W9e"
      },
      "source": [
        "We can see some variables are not relevant. Let's check the goodness of fit over the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQN8rVEc-TzP"
      },
      "source": [
        "# Predict over test set\r\n",
        "linear_pred_test = LGD_linear_model.predict(x_test)\r\n",
        "\r\n",
        "# Calculate the error\r\n",
        "linear_error = np.abs(linear_pred_test - y_LGD_test)\r\n",
        "\r\n",
        "# Print a scatter plot with distributions.\r\n",
        "fig, ax = plt.subplots(figsize=(11, 8.5))\r\n",
        "sns.scatterplot(x = y_LGD_test,            # The x is the real value\r\n",
        "                y = linear_pred_test,  # The y value is the predictor\r\n",
        "                hue = linear_error,    # The colour represents the error\r\n",
        "                legend = False\r\n",
        "                )\r\n",
        "\r\n",
        "# Overlay a diagonal line\r\n",
        "X_plot = np.linspace(0, 1, 100)\r\n",
        "Y_plot = X_plot\r\n",
        "\r\n",
        "plt.plot(X_plot, Y_plot, color='r')\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8kWdEpr_k5H"
      },
      "source": [
        "We can see several values are predicted to be below 0, while many are shown to be below its correct value, particularly for large graphs. How dark a point shows the error magnitud. Let's see the average error magnitud."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FjVPXQFDTt3"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\r\n",
        "\r\n",
        "linear_mse = mean_squared_error(y_LGD_test, linear_pred_test)\r\n",
        "print('The MSE for the linear model is %.4f' % linear_mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJajm_OExOJO"
      },
      "source": [
        "## General Linear Regression Fit\r\n",
        "\r\n",
        "General regressions are not implemented in Python yet. This means we should use the GLM trick that we saw in the lectures to estimate a regression model that has an appropriate output distribution. Let's see how this would work out.\r\n",
        "\r\n",
        "The first step is to look for the best distribution for our data. For this we can use the [```fitter```](https://github.com/cokelaer/fitter) package, that tries to find the best distribution among all available in scipy. Let's install it and load it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wczyeihp2jGW"
      },
      "source": [
        "!pip install fitter\r\n",
        "import fitter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iebre4IN5PoS"
      },
      "source": [
        "Now we can look for the best distribution. The process is:\r\n",
        "1.  Create the fitter object.\r\n",
        "2. Fit it over our LGD data.\r\n",
        "3. Pick the best distribution between all available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oThNl3VToPlE"
      },
      "source": [
        "# Generate the fitter object.\r\n",
        "dists_LGD = fitter.Fitter(LGD_data['LGD'],      # The data\r\n",
        "                          timeout = 30,         # How long to wait before timeout. Some distributions are very hard to fit!\r\n",
        "                          distributions = None, # Optionally you can give distributions. None means all of them, ironically.\r\n",
        "                          )\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_Z9dNRO5ifs"
      },
      "source": [
        "Not all distributions are good for our problem. This can greatly increase fitting time too. Let's restrict distributions to those we believe might be adequate for our case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHiOH8nLu-tW"
      },
      "source": [
        "# Get the full list of distributions.\r\n",
        "dists_LGD.distributions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tGU5SxYvD9M"
      },
      "source": [
        "# Pick a few.\r\n",
        "dists_LGD.distributions = ['beta', 'gamma', 'mielke', 'lognorm']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myeBlKIru9-f"
      },
      "source": [
        "# Fit it\r\n",
        "dists_LGD.fit(n_jobs = -1,      # How many cores to use.\r\n",
        "              progress = True  # Show progress bar\r\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Vqq4YuzrL97"
      },
      "source": [
        "dists_LGD.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKL3Gi_awXFs"
      },
      "source": [
        "We can see the best distributions are the Mielke distribution (a mix between a beta and an F function common in physical phenomena) and the gamma distribution, a generalization of the beta distribution.\r\n",
        "\r\n",
        "Let's use [Mielke's distribution](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mielke.html) for this example. Every dataset can have its own distribution!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZn0q88XrzHm"
      },
      "source": [
        "dists_LGD.get_best()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGqLV4YpySRW"
      },
      "source": [
        "The function for Mielke's density distribution are k, s, loc and scale, which are conveniently given in the above dataframe. We see there is no translation made (loc is close to 0), but we do need to scale the distribution a bit (the fourth parameter). The k and s parameters give the shape of the distribution. \r\n",
        "\r\n",
        "What functions do we need? Well, the general process for a regression of this type is:\r\n",
        "\r\n",
        "1. Get where on the original cumulative distribution a point (the LGD) falls. For this we need the cumulative Mielke distribution, called the ```cdf``` function in scipy.\r\n",
        "2. Get where on the normal distribution that particular point falls. For this we need the inverse of the cumulative function, also called the **percent point function** or ```ppf``` in scipy.\r\n",
        "3. Apply this to all points in the dataset. Now everything is mapped to a normal variable.\r\n",
        "4. Run a linear regression between our regressors and the z-transformed variable. You can use LASSO, ElasticNet, etc to get the best model.\r\n",
        "5. Go back. For this you need the inverse of the cumulative normal distribution function (cdf) and the inverse cumulative distribution function for our target distribution (Mielke's ppf function).\r\n",
        "\r\n",
        "Let's import all required functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQtm7F12w_MB"
      },
      "source": [
        "# Import the functions\r\n",
        "from scipy.stats import mielke, norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfXXfJdQ56nV"
      },
      "source": [
        "# Set the parameters to particular values.\r\n",
        "LGD_mielke = mielke(*dists_LGD.fitted_param['mielke'])\r\n",
        "LGD_normal = norm()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyOLEWXy8VNi"
      },
      "source": [
        "Let's begin the calculations. The first step is to get the CDF of all elements in the Mielke distribution and finding its corresponding z-value in the normal distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2bTnVAf8RGG"
      },
      "source": [
        "# Get the Mielke cdf point.\r\n",
        "LGD_data['MielkeCDF'] = [LGD_mielke.cdf(x) for x in LGD_data['LGD']]\r\n",
        "\r\n",
        "# Get the corresponding z-value in the normal function\r\n",
        "LGD_data['Z-Mielke'] = [norm.ppf(x) for x in LGD_data['MielkeCDF']]\r\n",
        "LGD_data['Z-Mielke'].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50opUBGy9JfN"
      },
      "source": [
        "Our data is perfectly mapped to a normal regression. Now we are ready to run the regression! We can use the same code as before, but our target now will be the newly calculate Z-Mielke variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSfFU9Ng9FaP"
      },
      "source": [
        "LGD_mielke_model = ElasticNetCV(l1_ratio=np.arange(0.01, 1.01, 0.05),  # l1_ratios to try.\r\n",
        "                                n_alphas=10,                        # How many alphas to try per l1_ratio\r\n",
        "                                fit_intercept=True,                 # Use constant?\r\n",
        "                                max_iter=1000,                      # Iterations\r\n",
        "                                tol=0.0001,                         # Parameter tolerance\r\n",
        "                                cv=3,                               # Number of cross_validation folds\r\n",
        "                                verbose=True,                       # Explicit or silent training\r\n",
        "                                n_jobs=2,                           # Cores to use\r\n",
        "                                random_state=20201209               # Random seed\r\n",
        "                                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0btkFRI-RM_"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "x_train, x_test, y_train, y_mielke_test = train_test_split( LGD_data.iloc[:, 0:13], # Predictors\r\n",
        "                                                    LGD_data['Z-Mielke'],         # Target variable\r\n",
        "                                                    test_size=0.33,          # Test size percentage\r\n",
        "                                                    random_state=20201209    # Seed\r\n",
        "                                                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5R6Kwhj-c2o"
      },
      "source": [
        "LGD_mielke_model.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M--M4_R-mQo"
      },
      "source": [
        "coef_df = pd.concat([pd.DataFrame({'column': x_train.columns}), \r\n",
        "                    pd.DataFrame(np.transpose(LGD_mielke_model.coef_))],\r\n",
        "                    axis = 1\r\n",
        "                   )\r\n",
        "\r\n",
        "coef_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3n2-j0q-vVk"
      },
      "source": [
        "Similar variables are relevant now, but the weights have clearly changed! We can now apply this model to the test data and then calculate the corresponding LGD by reversing our procedure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pPgB6hP-uFo"
      },
      "source": [
        "# Predict over test set\r\n",
        "mielke_pred_test = LGD_mielke_model.predict(x_test)\r\n",
        "mielke_pred_test = norm.cdf(mielke_pred_test)\r\n",
        "mielke_pred_test = LGD_mielke.ppf(mielke_pred_test)\r\n",
        "\r\n",
        "# Calculate the error\r\n",
        "mielke_error = np.abs(mielke_pred_test - y_LGD_test)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnrJtV8R_dpW"
      },
      "source": [
        "Now that we have the estimates and the error, we can plot our results and calculate the MSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veNyYE_n_iGo"
      },
      "source": [
        "\r\n",
        "# Print a scatter plot with distributions.\r\n",
        "fig, ax = plt.subplots(figsize=(11, 8.5))\r\n",
        "sns.scatterplot(x = y_LGD_test,            # The x is the real value\r\n",
        "                y = mielke_pred_test,  # The y value is the predictor\r\n",
        "                hue = mielke_error,    # The colour represents the error\r\n",
        "                legend = False\r\n",
        "                )\r\n",
        "\r\n",
        "# Overlay a diagonal line\r\n",
        "X_plot = np.linspace(0, 1, 100)\r\n",
        "Y_plot = X_plot\r\n",
        "\r\n",
        "plt.plot(X_plot, Y_plot, color='r')\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FztiCekAAlx"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\r\n",
        "\r\n",
        "linear_mse = mean_squared_error(y_LGD_test, mielke_pred_test)\r\n",
        "print('The MSE for the Mielke-distributed model is %.4f' % linear_mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2qHzEMbBHO3"
      },
      "source": [
        "So we got a lower error! The improvement is not extreme in this dataset, but besides getting a better error we also get a better distribution: Our model starts at 0 and covers most of the original range. We can use this trick to create a regression for any distribution we want. As an exercise, train an XGBoosting model for this data and compare it with our Mielke distributed model. Can you improve the MSE with a non-linear model?"
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab 4 - Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CBravoR/AdvancedAnalyticsLabs/blob/master/notebooks/python/Lab_4_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twKpLeRhDeyQ"
      },
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "In this lab, we will set up a **data pipeline** in order to leave the data ready for analysis. Data preprocessing tends to be, by far, the most time-consuming step of the data science process. Errors in this step propagate to the model, so it is really important we do this correctly.\n",
        "\n",
        "The goal of this step is to leave the date ready to apply models to it. \n",
        "\n",
        "Every problem has its own set of data preprocessing functions to apply it to, but we will focus on the ones most common in classification models. In general we want to:\n",
        "\n",
        "1. Eliminate redundant variables.\n",
        "2. Treat null values.\n",
        "3. Treat outliers.\n",
        "4. Remove correlated features.\n",
        "\n",
        "For this goal we will use the excellent [```scikit-learn```](https://scikit-learn.org/stable/) package, which comes with most data-intensive operations. We will also use today (and during the rest of the module) the [```pandas```](https://pandas.pydata.org/) package, which allows for data handling in general."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTKdVYZLGm-b"
      },
      "source": [
        "## Reading the data\n",
        "\n",
        "Let's download the Bankloan data. I have slightly modified it removing the PD and LGD information (as we will now estimate our own). As a reminder, it has the following variables:\n",
        "\n",
        "- Customer: ID, or unique label, of the borrower (NOT predictive).\n",
        "- Age: Age of the borrower in years.\n",
        "- Education: Maximum education level the borrower reached.\n",
        "1: Complete primary. 2: Completed Secondary. 3: Incomplete Higher Ed. 4: Complete Higher Ed. 5: With postgraduate studies (complete MSc or PhD).\n",
        "- Employ: Years at current job.\n",
        "- Address: Years at current address.\n",
        "- Income: Income in ‘000s USD.\n",
        "- Leverage: Debt/Income Ratio.\n",
        "- CredDebt: Credit card standing debt.\n",
        "- OthDebt: Other debt in ‘000s USD.\n",
        "- MonthlyLoad: Monthly percentage from salary used to repay debts.\n",
        "- Default: 1 If default has occurred, 0 if not (Target variable).\n",
        "\n",
        "We will download it now directly from a link, using the more ubiquitous [```wget```](http://www.gnu.org/software/wget/) command. The command is\n",
        "\n",
        "```\n",
        "wget [-options] path\n",
        "```\n",
        "\n",
        "We need to add the options ```--no-check-certificate``` and ```--output-document=FILENAME``` so it downloads ok."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3v98YWoGla0"
      },
      "source": [
        "!wget --no-check-certificate --output-document=Bankloan.csv 'https://docs.google.com/spreadsheets/d/1nUJ1fA5f1VeMvulknpsvxpy0GW3CekNnhgeLRK0WlDI/export?gid=1016776666&format=csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ExNpn4rMwfT"
      },
      "source": [
        "To check what we downloaded we can use the ```head``` OS command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oz4xRtSDM20o"
      },
      "source": [
        "!head Bankloan.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5sgZk3lMtIK"
      },
      "source": [
        "Now we will use Pandas to read the CSV file. The  function to do so is [```read_csv```](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html). We will store the results in a variable named ```bankloan_data```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wj-Yl2t5JW0Y"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "bankloan_data = pd.read_csv('Bankloan.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2A_hACkDcxY"
      },
      "source": [
        "As a reminder, here are the summary statistics of the variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpypXLF6Nm9B"
      },
      "source": [
        "bankloan_data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m5-Xr8lP5Gm"
      },
      "source": [
        "We can see there are a few null values in different variables and apparently an invalid outlier in Leverage.  Let's visualize the dataset using seaborn to get an idea of the distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnkTHoUiP4fj"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc-0gLkpPTgu"
      },
      "source": [
        "sns.set(color_codes=True)\n",
        "\n",
        "for col_id in bankloan_data.columns[np.r_[1,3:9]]:\n",
        "    sns.displot(data = bankloan_data, x = col_id, hue = \"Default\", kind = 'kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DohsQiAl0LJq"
      },
      "source": [
        "Seaborn can do many sophisticated (and aestetically pleasing) \n",
        "graphs. Go to [the Gallery](https://seaborn.pydata.org/examples/index.html) for details and example code. \n",
        "\n",
        "There are severe outliers in OthDebt, Leverage, Creddebt and Income, but we don't know which ones are valid and which invalid. We will treat these now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZLgIWHqSvts"
      },
      "source": [
        "## Data Cleaning\n",
        "\n",
        "Now we can focus on cleaning the data. Let's start with the easy part: removing null values. **Remember to check when an outlier is a missing value** (invalid outliers).\n",
        "\n",
        "### Null values\n",
        "\n",
        "The core function here will be Panda's [```fillna```](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html). This allows to replace all null values (represented by None or NaN in Python) by a certain value. This also allows to set what the replacement will be with the ```value``` argument. See the help for details.\n",
        "\n",
        "Remember the strategies to deal :\n",
        "\n",
        "1. Keep: If the null values are a category by themselves. In this case, replace by something meaningful.\n",
        "\n",
        "2. Delete: If the null values are too many **either by row or by column** then it is better to just drop the case or the variable.\n",
        "\n",
        "3. Replace: If there are only a few missings for the variable or the row (<1% total), replace by the  replace the null values by the **median** for continous variables, and the **mode** for categorical values.\n",
        "\n",
        "Let's study our dataset's null values. The [```isnull()```](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.isnull.html) function returns which elements in the dataframe are null. The [```any()```](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.any.html) function returns a list with whatever columns (default) or rows (passing ```axis = 1``` to the function) have any element with a boolean of true."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-s-kkifWlud"
      },
      "source": [
        "bankloan_data.isnull().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2Fp-LCjXv9s"
      },
      "source": [
        "We can see which columns have null values. Let's study them in further detail."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tgc0zy6UX04R"
      },
      "source": [
        "null_columns = bankloan_data.columns[bankloan_data.isnull().any()]\n",
        "bankloan_data[null_columns].isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ7_tgqsYLhj"
      },
      "source": [
        "Given the small number of cases for all variables except OthDebt, we can simply replace those values by the median.\n",
        "\n",
        "Let's study OthDebt cases more in detail."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uFhZbqGYUqp"
      },
      "source": [
        "bankloan_data.loc[bankloan_data.isnull().any(axis = 1), :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZWCGoKQYfvB"
      },
      "source": [
        "We can see all cases in which OthDebt is null. We can also see the egregious outlier that Leverage has, with a value of 9999999. These values give us some hints about what's happening with OthDebt. Let's study the minimum and maximum of the variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD5BqZ5hYwcY"
      },
      "source": [
        "bankloan_data.OthDebt.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpgxTyujY0YD"
      },
      "source": [
        "Aha! Note anything? There are no values equal to 0! We can make a good guess that the null values are equal to zero. In real life we would call the sysadmins to confirm this, but in this case we can make a guess.\n",
        "\n",
        "Then, we now should fix these values as follow:\n",
        "\n",
        "1. Replace the leverage equal to 999999 by a null value.\n",
        "2. Replace all OthDebt nulls by 0.\n",
        "3. Replace the remaining null values by the median."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Geu-R8RRa1Tf"
      },
      "source": [
        "# Replace invalid outlier.\n",
        "bankloan_data.Leverage.values[bankloan_data.Leverage.values == 999999] = np.NaN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFaxJbmNKGcN"
      },
      "source": [
        "# Fills out the null values with zeros. Inplace argument changes dataframe.\n",
        "bankloan_data.OthDebt.fillna(value = 0, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R06ZbW0DcN6f"
      },
      "source": [
        "# Fill out remaining elements.\n",
        "bankloan_data.fillna(bankloan_data.median(), inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgFDQXyBiHiU"
      },
      "source": [
        "bankloan_data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwY4w7DAcwyE"
      },
      "source": [
        "There are no more null values! We are now ready to study the distributions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hpzkfzwiPyB"
      },
      "source": [
        "### Outlier cleaning\n",
        "\n",
        "Cleaning outlier, on the other hand, requires a much more manual intervention. We know there are three variables we need to intervene in: Income, Creddebt, and OthDebt. In general, we would like to modify all cases where there is either a discontinuous distribution or a case that is outside of 3 to 6 standard deviations.\n",
        "\n",
        "It is very important to use your judgement in this! Don't just cut in 3 stds, as that is too restrictive in an exponential distribution. If you want to get an idea of the values, then seaborn can help. The function ```displot``` with either the option ```hist``` or ```kde``` allows to check the distribution of values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCOmLnVpiOVo"
      },
      "source": [
        "sns.displot(bankloan_data['Income'], kind = 'hist')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCvswT9-jagR"
      },
      "source": [
        "sns.displot(bankloan_data['Creddebt'], kind = 'kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPA95piMjhLv"
      },
      "source": [
        "sns.displot(bankloan_data['OthDebt'], kind = 'kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtGBJQw6jnmm"
      },
      "source": [
        "All variables need to be trimmed from the right. Potential cutoffs are Income of 300, Creddebt of 15, and Othdebt of 30.\n",
        "\n",
        "To actually cut the values, then we can simply write a proper expression in Pandas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWhH4fgcji1G"
      },
      "source": [
        "bankloan_data = bankloan_data.loc[(bankloan_data['Income'] < 300) & (bankloan_data['Creddebt'] < 15) & (bankloan_data['OthDebt'] < 30)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpbVxiJQmOBs"
      },
      "source": [
        "We can check now how the data looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgUVYbTKlpQT"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "a = sns.violinplot(x='variable', y='value', data=pd.melt(bankloan_data.iloc[:, np.r_[1,3:9]]), ax=ax)\n",
        "a.set_xticklabels(a.get_xticklabels(), rotation=90);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPDWYQFymRX1"
      },
      "source": [
        "Much better, although income still has a large outlier. This plot might be misleading though, as the magnitud of the data is relevant. We might want to recheck this after normalizing the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwSEaeGlmFzO"
      },
      "source": [
        "## Normalization\n",
        "\n",
        "Most models require the inputs to be in the same scale, this is called **normalization**. It is very important for most models... except for credit scoring as we will use Weight of Evidence (see the lecture!). Still, this is fairly simple to do in Pandas, being smart about what columns we select and **mapping** a function to those columns.\n",
        "\n",
        "The most traditional mapping is the zscore. We can use scipy's version of it. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E_PSudynST3"
      },
      "source": [
        "from scipy.stats import zscore"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEFMdbTMnW68"
      },
      "source": [
        "We now select all numeric columns, except the customer one. We can do this with ```select_dtypes```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0I8jDLlm6vM"
      },
      "source": [
        "# Identify the numerical columns\n",
        "numeric_cols = bankloan_data.select_dtypes(include=[np.number]).columns\n",
        "numeric_cols "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAmoNyjxny0G"
      },
      "source": [
        "# Remove the first and last one\n",
        "numeric_cols = numeric_cols[1:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1ETL7-Nn11u"
      },
      "source": [
        "# Apply the zscore function to all data\n",
        "bankloan_data[numeric_cols] = bankloan_data[numeric_cols].apply(zscore)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StvuqRWApixH"
      },
      "source": [
        "Now the data should look much better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrahcfLIoN45"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "a = sns.violinplot(x='variable', y='value', data=pd.melt(bankloan_data.iloc[:, np.r_[1,3:9]]), ax=ax)\n",
        "a.set_xticklabels(a.get_xticklabels(), rotation=90);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9o6wiTXdlsv"
      },
      "source": [
        "The dataset looks really clean now. We are ready to use further models! To be 100% purist **you should first split between train and test set** before doing all of these analyses (so you should use the median of the train set). Please remember that when solving your coursework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7F6iCmc-oTb_"
      },
      "source": [
        "## Self-study: Dummy coding\n",
        "\n",
        "The final step will be transform the categorical variables to dummy variables. Again, this is **not necessary for credit scoring** as the WoE transform is much more robust. The best strategy here is to follow the 5% rule:\n",
        "\n",
        "1. Aggregate by expert judgement (i.e. your opinion) until each group has more than 5% of data (1% if very large data) and there are all classes (by target variable) present.\n",
        "\n",
        "2. Use target variable percentage (or classification trees) until you get a proper number of groups.\n",
        "\n",
        "To check the number of cases for each default level, we can calculate a crosstab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tnu4zQW0oSMP"
      },
      "source": [
        "pd.crosstab(bankloan_data['Education'], bankloan_data['Default'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ_jTjWOrw-o"
      },
      "source": [
        "Now we can calculate the mean of each educational level."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xFFkNvLqz61"
      },
      "source": [
        "bankloan_data.groupby('Education', as_index=False).agg({'Default': [\"mean\"]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ__rGzgskpG"
      },
      "source": [
        "We could, if we wanted to, group together some educational levels, such as '1upInc', 'Med', and 'Posg'. If not, we can create dummy variables directly with Pandas ```get_dummies```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsntGOQ4sjtN"
      },
      "source": [
        "bankloan_data = pd.get_dummies(bankloan_data)\n",
        "bankloan_data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCaA3YQStIc2"
      },
      "source": [
        "Now we are ready to apply general models!"
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab 9 - Embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CBravoR/AdvancedAnalyticsLabs/blob/master/notebooks/python/Lab_9_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwgnoEJt2iE_",
        "colab_type": "text"
      },
      "source": [
        "# Embeddings\n",
        "\n",
        "In this lab we will use an embedding to train a simple model over the IMDB dataset, but we will use an embedding instead of a one-hot representation. For this, we will use the fastText embeddings that have been provided.\n",
        "\n",
        "First, let's start by importing the data and the packages that we will use. Remember to set the runtime environment to GPU!\n",
        "\n",
        "## Data Import"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjJ8YJnwaoFg",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_layVxFU2_6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# General imports\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn.feature_extraction as skprep\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from itertools import compress\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "random.seed(20190124)\n",
        "%matplotlib inline\n",
        "\n",
        "# Keras imports\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Reshape, MaxPooling1D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, Lambda\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.optimizers import SGD, RMSprop, Adam\n",
        "from tensorflow.keras.metrics import categorical_crossentropy, categorical_accuracy\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.preprocessing import image, sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__0Lf_Hv3EAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download a zip file with the data\n",
        "!wget --no-check-certificate --output-document=IMDB.zip 'https://drive.google.com/uc?export=download&id=1owCcH4eU_XvUzrjnMVec-obX3endPzy_'\n",
        "\n",
        "# Extract the files.\n",
        "!unzip IMDB.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNRQEl1f6PJz",
        "colab_type": "text"
      },
      "source": [
        "With this we are ready to start training embeddings!\n",
        "\n",
        "# fasttext\n",
        "\n",
        "We will use [fasttext embeddings](https://github.com/facebookresearch/fastText) in this lab. For this, we need to download the fasttext model and apply it to our data, i.e., associate each word with the corresponding embedding vector.\n",
        "\n",
        "fasttext is a heavy program, so it makes sense to use the C++ library directly. This does complicate our life a bit, but nothing a few lines of code can't solve. First, we will download the library and unzip it as before.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHzjAjgB6BMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://github.com/facebookresearch/fastText/archive/v0.9.1.zip\n",
        "  \n",
        "!unzip v0.9.1.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAENl0CT7_UC",
        "colab_type": "text"
      },
      "source": [
        "Now we need to [compile](https://en.wikipedia.org/wiki/Compiler) the library. Compiling turns the code we just downloaded to something the computer can understand. Configuring this is complicated, so programmers add a [makefile](http://www.cs.colby.edu/maxwell/courses/tutorials/maketutor/) with instructions for the compiler. This means we just need to call the command ```make``` in the base folder with the code. This code does it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WfqT14678gq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd fastText-0.9.1\n",
        "\n",
        "!make"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2I4kf32v_kFc",
        "colab_type": "text"
      },
      "source": [
        "There are some warnings, but we can ignore them as they are for future versions.\n",
        "\n",
        "Now, we need to download the embedding vectors. These are **really heavy downloads** of about 8GB.  fasttext is a language-dependent model, so be sure to download the one for your chosen application. The list is [here](https://fasttext.cc/docs/en/english-vectors.html). There are four levels of embeddings:\n",
        "\n",
        "- Vectors trained over Wikipedia (1 million words, 16 billion tokens).\n",
        "- Vectors trained over Wikipedia with subword information.\n",
        "- Vectors trained over a webcrawl of many websites ([Common Crawl](http://commoncrawl.org/)). This one has 2M words and 200B tokens.\n",
        "- Vectors trained over a webcrawl of many websites with subword information. \n",
        "\n",
        "For the lab we will use the first one, but I encourage you to use try other ones! The download will take a little while."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AYbQEvv_aJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
        "  \n",
        "!gunzip -v -f cc.en.300.bin.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtAb3nGqEG1X",
        "colab_type": "text"
      },
      "source": [
        "Now we are ready to go!\n",
        "\n",
        "## Data Preprocessing\n",
        "\n",
        "With this ready we can now start working on the data, which comes from the [Internet Movie Database](https://www.imdb.com/) (IMDB). The following code reads the data in the format it comes, which is the folder 'IMDB_Lecture_Sample' with two folders: pos and neg with positive and negative reviews. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgDDmGJhFxl8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Come back to the original work folder\n",
        "%cd /content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmetbpm7C7gE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import relevant packages\n",
        "import os\n",
        "import codecs\n",
        "import pandas as pd\n",
        "\n",
        "# List all files in the \"pos\" directory. Replace with your own!\n",
        "dir = 'IMDB_Lecture_Sample/train/pos/'\n",
        "fileList = os.listdir(dir)\n",
        "\n",
        "# Create vector with texts\n",
        "outtexts = []\n",
        "\n",
        "# Read the files in the directory and append them with the class to the dataset\n",
        "for eachFile in fileList:\n",
        "    with codecs.open(dir + eachFile, encoding='utf-8') as _fp:\n",
        "        fileData = _fp.read()\n",
        "        outtexts.append(fileData)\n",
        "    _fp.close()\n",
        "    \n",
        "# Create dataframe from outputs\n",
        "texts = pd.DataFrame({'texts': outtexts, 'class': 1})\n",
        "\n",
        "# Repeat for negative values\n",
        "# List all files in the \"pos\" directory\n",
        "dir = 'IMDB_Lecture_Sample/train/neg/'\n",
        "fileList = os.listdir(dir)\n",
        "\n",
        "# Create vector with texts\n",
        "outtexts = []\n",
        "\n",
        "# Read the files in the directory and append them with the class to the dataset\n",
        "for eachFile in fileList:\n",
        "    with codecs.open(dir + eachFile, encoding='utf-8') as _fp:\n",
        "        fileData = _fp.read()\n",
        "        outtexts.append(fileData)\n",
        "    _fp.close()\n",
        "    \n",
        "# Create dataframe from outputs\n",
        "texts = pd.concat((texts, pd.DataFrame({'texts': outtexts, 'class': 0})), ignore_index = True)\n",
        "\n",
        "texts.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5OWCnSmhi3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1Tx9uXbGOkQ",
        "colab_type": "text"
      },
      "source": [
        "Now we'll clean the text, getting rid of special characters and keeping the text in a clear form. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Q8yPXEIFn-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Text cleaning\n",
        "import string\n",
        "\n",
        "# Collect punctuation signs.\n",
        "table = str.maketrans(' ', ' ', string.punctuation)\n",
        "\n",
        "# Remove them from the text\n",
        "texts.iloc[:,0] = [j.translate(table) for j in texts.iloc[:,0]]\n",
        "texts.iloc[:,0] = [j.replace('\\x96',' ') for j in texts.iloc[:,0]]\n",
        "\n",
        "# Eliminate double spaces\n",
        "texts.iloc[:,0] = [\" \".join(j.split()) for j in texts.iloc[:,0]]\n",
        "\n",
        "# Show first 5\n",
        "texts.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bN6R_l89HKeJ",
        "colab_type": "text"
      },
      "source": [
        "## Estimating the embedding\n",
        "\n",
        "Once we have the embedding model, using it consists of:\n",
        "\n",
        "1. Calculate the words that appear on the text and save to disk\n",
        "2. Use the fastText program to obtain the word embeddings.\n",
        "3. Import the embeddings into a Keras input layer.\n",
        "4. Train the model!\n",
        "\n",
        "First, we will start by selecting the individual words. The Keras internal model \"[Tokenizer](https://keras.io/preprocessing/text/#tokenizer)\" will allow us to quickly do this, with the added benefit of giving us a [dictionary](https://docs.python.org/2/tutorial/datastructures.html#dictionaries) of the words, which will be stored in the \"tokenizer\" model.\n",
        "\n",
        "A dictionary is a very powerful object included by Python, which will efficiently index anything by any key. In our case it will index the words and an arbitraty number that will give its position. Read the linked article to know more about dictionaries, but it is important that you understand its usefulness: It allows fast (indexed) access to objects linked by a key (in this case, the words)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6owu-t2G0Z7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer() # Creates tokenizer model.\n",
        "tokenizer.fit_on_texts(texts.iloc[:,0]) # Trains it over the tokens that we have.\n",
        "\n",
        "# Get words\n",
        "Vals = list(tokenizer.word_index.keys())\n",
        "\n",
        "# Write CSV with the output.\n",
        "file = codecs.open('IMDBWords.csv', \"w\", \"utf-8\")\n",
        "\n",
        "for item in Vals:\n",
        "    file.write(\"%s\\r\\n\" % item)\n",
        "    \n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBLb3CX9Hiyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hqm-JobbHzXH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head IMDBWords.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eR7L1c4pHse0",
        "colab_type": "text"
      },
      "source": [
        "We now have a csv file with all the words being used to review the movies in a standard format. Let's get the embeddings!\n",
        "\n",
        "We need to call the fasttext software from the command line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb79GHksIKlY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwRTWVwWH6GK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!./fastText-0.9.1/fasttext print-sentence-vectors fastText-0.9.1/cc.en.300.bin < IMDBWords.csv > EmbeddingIMDB.tsv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UwGcBfEIlbe",
        "colab_type": "text"
      },
      "source": [
        "As always, ignore the warnings. They are basically saying \"I need a very large amount of RAM to do what you are asking!\".\n",
        "\n",
        "This process actually takes a relatively long time. Let's take a look at the command part by part:\n",
        "\n",
        "- ```!./fastText-0.2.0/fasttext``` invokes fasttext. The notation \"./\" means \"execute this program\".\n",
        "\n",
        "- We give it two parameters ```print-sentence-vectors``` which instruct fastText to actually give us the embedding for every word, and ```fastText-0.2.0/cc.en.300.bin``` which is the language model we are using.\n",
        "\n",
        "- Then comes the processing of the inputs and outputs. The \"```< IMDBWords.csv```\" is telling Linux \"give IMDBWords.csv as an input to what's to the left\" and the \"```> EmbeddingIMDB.tsv```\" is telling Linux \"write whatever is outputted from the left into EmbeddingIMDB.tsv\".\n",
        "\n",
        "The output is a space-separated file with the embedding vectors in the same order we gave them to the software."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OV434-joIQSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head EmbeddingIMDB.tsv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zls3x2d7K1fs",
        "colab_type": "text"
      },
      "source": [
        "Note that this is only for training, for testing we would:\n",
        "\n",
        "- If we kept the embedding as is, we simply calculate the new embeddings for the new words and add it to our matrix.\n",
        "\n",
        "- If we retrained the embeddings, then we would either use the output that we already have if the word was in our original vocabulary, or just leave a vector of zeros for those words if it is not.\n",
        "\n",
        "fastText outputs space-separated words. We replace them with a comma."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SVWsxocKfG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import fileinput\n",
        "\n",
        "with fileinput.FileInput('EmbeddingIMDB.tsv', inplace=True, backup='.bak') as file:\n",
        "    for line in file:\n",
        "        print(line.replace(' ', ','), end='')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uket1APpLG0O",
        "colab_type": "text"
      },
      "source": [
        "We add a first line with the variable names, to be able to import it back."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMI4rtsMLCxD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Create the first line\n",
        "firstLine = ','.join(['D'+str(i) for i in np.arange(1, 301)]) + '\\n'\n",
        "\n",
        "# Open as read only. Read the file\n",
        "with open('EmbeddingIMDB.tsv', 'r') as original: \n",
        "  data = original.read()\n",
        "\n",
        "# Open to write and write the first line and the rest\n",
        "with open('EmbeddingsIMDB.csv', 'w') as modified: \n",
        "  modified.write(firstLine + data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cq-vQoKFLbca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head EmbeddingsIMDB.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuFMUUKwLhcv",
        "colab_type": "text"
      },
      "source": [
        "Just what we wanted! Now we have a matrix with every word in the document with its corresponding Embedding. We can now import this file into Python, and use it to train our model.\n",
        "\n",
        "## Using the Embedding Layer\n",
        "\n",
        "The next step is to actually train a neural network with an Embedding Layer. For this, Keras has the aptly named \"Embedding\" layer, which will take care of our structures. The following code creates a very simple network that does the following:\n",
        "\n",
        "1. Read the embeddings.\n",
        "2. Calculate the One-Hot inputs (by using an \"index\") which will index which words are in which text.\n",
        "3. Create a layer that associates the indexes with the embeddings.\n",
        "4. Create the rest of the architecture.\n",
        "5. Train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-VLJQqOLftz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read word embeddings\n",
        "Embeddings = pd.read_csv('EmbeddingsIMDB.csv', sep=',', decimal = '.', \n",
        "                         low_memory = True, index_col = False)\n",
        "Embeddings.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRdxmBYFL221",
        "colab_type": "text"
      },
      "source": [
        "We will now create a dictionary for the embeddings. The zip function allows to create the (key, element) structure that we need. Read more about the zip function [here](https://docs.python.org/3.7/library/functions.html#zip). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3euIHnaCLydM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create embedding dictionary\n",
        "\n",
        "EmbeddingsDict = dict(zip(Vals, Embeddings.values))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ60UOCiL-vB",
        "colab_type": "text"
      },
      "source": [
        "Now, let's study our texts to create the optimal embedding layer. One of the decisions we need to make is what is going to be the maximum size of our documents. Too large, and we will need to add a lot of padding thus will make it inefficient; too small, and we will be losing a lot of information. There is no clear rule here, I usually try to cover 90% of all elements, but you can argue anything that makes sense to you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JEHS-AiL8o7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "# Count maximum number of words per file.\n",
        "wordDist = [len(w.split()) for w in texts.iloc[:,0]]\n",
        "print('Avg. no of words: ' + str(np.round(np.mean(wordDist), 2)))\n",
        "print('Std. deviation: ' + str(np.round(np.std(wordDist), 2)))\n",
        "print('Max words: ' + str(np.max(wordDist)))\n",
        "\n",
        "# Generate the plot\n",
        "distIMDB = sns.distplot(wordDist)\n",
        "\n",
        "# I'm saving the image to a PDF, as it makes it easier later to download.\n",
        "distIMDB.figure.savefig(\"wordDist.pdf\", format = \"pdf\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9ESSnPPMRpZ",
        "colab_type": "text"
      },
      "source": [
        "Arbitrarily, we will use 600 words maximum. Try different values!\n",
        "\n",
        "Now we create the input layer. The first layer will have the index of each word per-text, which then we will use to efficiently associate with the embedding. For this, we use Keras' \"pad_sequence\". This will either add padding to texts that are smaller than 600, or trim the ones that are longer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQa-FIU-MKrk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create word index from input\n",
        "sequences = tokenizer.texts_to_sequences(texts.iloc[:,0]) # Create the sequences.\n",
        "\n",
        "# Creates the indexes. Word index is a dictionary with words in it.\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "# Creates the training dataset, adding padding when necessary.\n",
        "data = pad_sequences(sequences, maxlen=600, \n",
        "                     padding = 'post') # add padding at the end. No difference in practice.\n",
        "\n",
        "# Creates the objective function\n",
        "labels = texts.iloc[:,1]\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8VsMe2RMUVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's save the outputs, so we don't run all of the above 20 times.\n",
        "# Be efficient! Save always save intermediate outputs\n",
        "\n",
        "# Create saving directory\n",
        "!mkdir IMDB_Preprocessed\n",
        "\n",
        "# Save outputs\n",
        "np.savetxt(\"IMDB_Preprocessed/IMDB_Padded.txt\", data)\n",
        "np.savetxt(\"IMDB_Preprocessed/IMDB_Labels.txt\", labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptXIANqaMhWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB8hcqj3MkCT",
        "colab_type": "text"
      },
      "source": [
        "As we can see above, our data now is a matrix corresponding to where on the embedding matrix is the vector we are looking for. This is an extremely efficient way of storing embeddings, but uses more CPU. That's ok though!\n",
        "\n",
        "Now we are almost ready! Now we need to construct the Embedding matrix. This matrix will have the weights associated with each index. Keras will automatically construct the correct embedding of length 600 (see below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V86-BaoCMixC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create first matrix full with 0's\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "\n",
        "# Generate embeddings matrix\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = EmbeddingsDict.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Print what came out\n",
        "embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91cGUOqsNajk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Again, we save the intermediate result. If done right you only need this matrix!\n",
        "# No need to run everything all over again.\n",
        "np.savetxt(\"IMDB_EmbeddingMatrix.txt\", embedding_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSub9yIeaY70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will also save the word dictionary\n",
        "# A pickle file is a Python native file\n",
        "import pickle\n",
        "f = open(\"WordDictionary.pkl\",\"wb\")\n",
        "pickle.dump(word_index, f)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_VV90nlSUFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Zip all files for download.\n",
        "!zip -r IMDB_Preprocessed.zip IMDB_Preprocessed "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-TWPGHBaTLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download files\n",
        "from google.colab import files\n",
        "files.download(\"IMDB_Preprocessed.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "548hhdrfRXm-",
        "colab_type": "text"
      },
      "source": [
        "## Modelling using an embedding layer\n",
        "\n",
        "Now that we have this ready, we need to create our model and add an [Embedding Layer](https://keras.io/layers/embeddings/). We'll create a very simple model using Convolutional Layers as hidden layers. In the next lecture we'll check in detail what this means."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73fUd6CDRWz-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Final model.\n",
        "model = Sequential()\n",
        "embedding_layer = Embedding(len(word_index) + 1,           # Words in the embedding.\n",
        "                            300,                           # Embedding dimension\n",
        "                            weights=[embedding_matrix],    # The weights we just calculated\n",
        "                            input_length=600,              # The maximum number of words.\n",
        "                            trainable=False)               # To NOT recalculate weights!\n",
        "\n",
        "model.add(embedding_layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQYe-xGlR-_r",
        "colab_type": "text"
      },
      "source": [
        "Very important: If you are letting your embedding to adapt to your own model, you need to set \"trainable=True\", if not, leave to False.\n",
        "\n",
        "Done! We have a model that uses an embedding layer as input. Let's try it in a (very bad) model.Our network will take the embedding as input and will estimate the probability of being of class 1 or 0 (positive or negative). A potential architecture is as follows:\n",
        "\n",
        "- A [1D-Convolutional Layer](https://keras.io/layers/convolutional/): See the next lecture for details :). I will add 64 filters and a kernel size of 3, which means \"look for 64 different combinations of 3 words that are useful\". We use ReLU activation for it.\n",
        "\n",
        "- A [Flatten](https://keras.io/layers/core/#flatten) layer: The embedding matrix comes as a, well, a matrix, the output of the first layer will be as well. We need to change this to a shallow 1D tensor. The Flatten layer takes matrices (or N-Dimensional tensors) and turns them into 1D tensors.\n",
        "\n",
        "- A Dense layer with 64 neurons and ReLU activation.\n",
        "\n",
        "- A [Dropout](https://keras.io/layers/core/#dropout) layer: Big models can have many millions of parameters. These models are prone to be overfitted. [Srivastava et al. (2014)](http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf) realized that a simple way to avoid overadjustment was to simply randomly set a large number of parameters to 0. This is called \"Dropout\". We will randomly set 40% of all weights to 0. This is a tunable parameter, you should experiment with parameters that make sense to you.\n",
        "\n",
        "- A sigmoid output layer, with 1 neuron. As this is a binary problem, that's the most appropriate one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gf6bNhnPRSAE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check for 64 sequences of length 3.\n",
        "model.add(Conv1D(64, 3, activation = 'relu'))\n",
        "\n",
        "# Turn output matrices into 1D tensor for shallow network.\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add 64 neurons with ReLU activation.\n",
        "model.add(Dense(64))\n",
        "\n",
        "# Add dropout.\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Add an output layer with a sigmoid.\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAF8pfDvTjZM",
        "colab_type": "text"
      },
      "source": [
        "As this is a binary classification problem, we need a binary cross-entropy error function. I will use the optimizer [Adam](https://arxiv.org/abs/1412.6980) by Kingman et al. (2014), which works well for this problem and, more importantly, requires little tuning. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE0plUHMTj0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use Adam as optimizer, with a binary_crossentropy error.\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhVjsP7JTsJL",
        "colab_type": "text"
      },
      "source": [
        "Done! Let's see the architecture of our network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3QrRLNqTlbD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dutuCWHcTvws",
        "colab_type": "text"
      },
      "source": [
        "The model has around 2.5 million trainable parameters, the rest come from the embedding, which we left unchanged. Quite an increase compared to our last model!\n",
        "\n",
        "Now we train. We will use 33% of the data as a test set, and train for 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHIq1ib4Ttrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit the model\n",
        "history = model.fit(data, labels, validation_split=0.33, epochs=10, batch_size=20)\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCqHmoyvbObP",
        "colab_type": "text"
      },
      "source": [
        "We have clear signs the model overadjusted, but this is to be expected in data so simple. That's it! Now we can garnish the power of embeddings for modelling. We just need to learn to create models that can leverage this power. Remember, fastText is available for several hundreds of languages, so they can be used in many contexts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeZukXEyb6i2",
        "colab_type": "text"
      },
      "source": [
        "## Self-study\n",
        "\n",
        "Change the embedding layer and train your own embeddings. Do you get an improvement?\n",
        "\n",
        "Try with different architectures to get a better value in the validation layer. Why do you get such low scores? Next lecture we will study architectures that deal correctly with these many inputs.\n",
        "\n",
        "Compare the convergence speed of Adam with SGD with a reasonable learning rate (try very tiny values). Which one converges faster?\n",
        "\n",
        "### Other other embeddings\n",
        "In the coursework you are asked to use more than one embedding. Go through the tutorials for \n",
        "\n",
        "- GloVe: Keras has its own tutorial [here](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html).\n",
        "\n",
        "- BERT: This one, as is so new, is far more advanced. You need to install a few new packages to make it work. Read [this](https://github.com/hanxiao/bert-as-service) or [this](https://pypi.org/project/keras-bert/).\n",
        "\n",
        "### Categorical Embeddings\n",
        "\n",
        "Additionally, embeddings can also be used to efficiently encode categorical variables. [Read this](https://towardsdatascience.com/deep-embeddings-for-categorical-variables-cat2vec-b05c8ab63ac0)."
      ]
    }
  ]
}
